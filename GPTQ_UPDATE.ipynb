{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYPMXa55ipbx"
      },
      "source": [
        "GPTQ = original algorithm/paper (ETH Zurich, 2022).\n",
        "\n",
        "AutoGPTQ = practical Python library (Hugging Face maintained) that made GPTQ mainstream and easy to use.\n",
        "\n",
        "https://github.com/AutoGPTQ/AutoGPTQ\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGnAe6UG43Ba"
      },
      "source": [
        "| Feature / Library       | **AutoGPTQ**                          | **bitsandbytes**                                      | **gptqmodel**                                                  |\n",
        "| ----------------------- | ------------------------------------- | ----------------------------------------------------- | -------------------------------------------------------------- |\n",
        "| **Origin**              | Hugging Face ecosystem (2023)         | Tim Dettmers (2022)                                   | ModelCloud (2024–2025)                                         |\n",
        "| **Quantization Method** | GPTQ (post-training, 4/8-bit)         | Linear quantization (INT8, NF4, FP4, 8-bit optimizer) | GPTQ v1, GPTQ v2, QQQ, EoRA, GAR                               |\n",
        "| **Target Use**          | Easy Hugging Face integration         | Training & inference memory savings                   | **Full production deployment toolkit**                         |\n",
        "| **Hardware Support**    | CUDA (Nvidia GPU)                     | CUDA (Nvidia GPU) only                                | CUDA (Nvidia), ROCm (AMD), XPU (Intel), MPS (Apple), CPU       |\n",
        "| **Integration**         | Hugging Face Transformers             | PyTorch Optimizer + HF integration                    | HF Transformers, vLLM, SGLang, Optimum, Peft                   |\n",
        "| **Inference Kernels**   | ExLlama, Triton, Marlin (via plugins) | cuBLAS-based INT8 kernels                             | Marlin, ExLlama v2, Torch fused, BitBLAS                       |\n",
        "| **Training Support**    | No                                    | Optimizer states (8-bit Adam, NF4 LoRA)             | Partial (LoRA + EoRA fine-tune on quantized model)             |\n",
        "| **Flexibility**         | Focused on GPTQ only                  | Training + inference memory efficiency                | **Dynamic per-layer configs, adapters, GAR, eval integration** |\n",
        "| **Evaluation**          | None built-in                         | None built-in                                         | Built-in `lm-eval` & `evalplus` hooks                          |\n",
        "| **Ease of Use**         | Very easy (HF-style API)            | Very easy (drop-in optimizer / load\\_in\\_8bit=True) | More advanced config, but still has high-level API             |\n",
        "| **Community Models**    | Huge (TheBloke, HF Hub)               | Many LoRA + finetune models                           | Growing rapidly (ModelCloud & HF Hub vortex/EoRA releases)     |\n",
        "| **Strengths**           | Easy Hugging Face usage               | Simple + effective for training                       | All-in-one production toolkit, multi-hardware                  |\n",
        "| **Weaknesses**          | Limited to GPTQ only                  | Nvidia-only, no GPTQ                                  | More complex, newer ecosystem (still maturing)                 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC2kYd0yOqm3"
      },
      "source": [
        "Versions & Techniques\n",
        "\n",
        "GPTQ v1, GPTQ v2 → GPTQ ke alag implementations / improvements\n",
        "\n",
        "QQQ → Quick Quantization for Transformers (speed optimized variant)\n",
        "\n",
        "EoRA → Efficient Online Row-wise Approximation (better per-row error handling)\n",
        "\n",
        "GAR → Gradient Aware Rounding (quantization ke liye advanced rounding strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00n9egXp46Dq"
      },
      "source": [
        "https://github.com/ModelCloud/GPTQModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKecg9Fw4q_x"
      },
      "source": [
        "pip install gptqmodel → install the gptqmodel package.\n",
        "\n",
        "-v → verbose mode, so pip prints more details about what it is doing (downloads, builds, etc.).\n",
        "\n",
        "--no-build-isolation → disables pip’s default build isolation behavior when installing from source (sdist). It means pip will not create an isolated environment to build dependencies; you’re responsible to have all build dependencies present already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvzUv_8kiyIo"
      },
      "outputs": [],
      "source": [
        "!pip install -v gptqmodel --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAzww8Jr5Mw3"
      },
      "outputs": [],
      "source": [
        "!pip install \"protobuf<6.30\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQrqnBUYi31p"
      },
      "outputs": [],
      "source": [
        "from gptqmodel import GPTQModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8XKNrhux5I9"
      },
      "outputs": [],
      "source": [
        "import gptqmodel\n",
        "dir(gptqmodel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYETuNOu5_b_"
      },
      "source": [
        "https://huggingface.co/collections/ModelCloud/vortex-673743382af0a52b2a8b9fe2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Sb8ook5hM7"
      },
      "outputs": [],
      "source": [
        "model = GPTQModel.load(\"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v2.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9ni531h6EVd"
      },
      "outputs": [],
      "source": [
        "result = model.generate(\"Uncovering deep insights begins with\")[0] # tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEoHdrMgjPHs"
      },
      "outputs": [],
      "source": [
        "print(model.tokenizer.decode(result)) # string output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afYg85UpUFGB"
      },
      "source": [
        "<|begin_of_text|>Uncovering deep insights begins with a deep understanding of the underlying principles and concepts that govern the behavior of the system in question. In"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GclAJE-GkoSq"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq5IFsz-kpl-"
      },
      "outputs": [],
      "source": [
        "quant_path = \"Llama-3.2-1B-Instruct-gptqmodel-4bit\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0uUpTAUkr5m"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "calibration_dataset = load_dataset(\n",
        "    \"allenai/c4\",\n",
        "    data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
        "    split=\"train\"\n",
        "  ).select(range(1024))[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfVL-6S6q_i"
      },
      "outputs": [],
      "source": [
        "from gptqmodel import GPTQModel, QuantizeConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g1JzZyVk9My"
      },
      "outputs": [],
      "source": [
        "quant_config = QuantizeConfig(bits=4, group_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlKaeFPOk_SC"
      },
      "outputs": [],
      "source": [
        "model = GPTQModel.load(model_id, quant_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrjeDiMp7BVl"
      },
      "source": [
        "QuantizeConfig Parameters Explained\n",
        "\n",
        "bits: int = 4\n",
        "Number of bits for quantization (2, 3, 4, 8). Lower bits = more compression, less accuracy.\n",
        "\n",
        "dynamic: Dict[...] | None\n",
        "Allows per-layer overrides. Example: quantize some layers with 8 bits, others skip quantization.\n",
        "\n",
        "group_size: int = 128\n",
        "Number of weights grouped together before quantization.\n",
        "Smaller = better accuracy, but slower. Larger = faster, more compression.\n",
        "\n",
        "damp_percent: float = 0.05\n",
        "Used in Hessian damping (numerical stability). Prevents division by very small numbers.\n",
        "\n",
        "damp_auto_increment: float = 0.01\n",
        "If quantization fails for a layer, damping value is automatically increased by this amount.\n",
        "\n",
        "desc_act: bool = True\n",
        "Whether to use activation ordering (descending importance) for better accuracy.\n",
        "If False, disables this reordering.\n",
        "\n",
        "act_group_aware: bool = False\n",
        "(GAR feature) Group-aware reordering. Preserves activation sensitivity per group.\n",
        "Improves accuracy for grouped quantization.\n",
        "\n",
        "static_groups: bool = False\n",
        "If True, fixes the grouping instead of dynamic grouping.\n",
        "\n",
        "sym: bool = True\n",
        "Symmetric quantization (weights centered around zero).\n",
        "If False, asymmetric (different zero-point for positive/negative).\n",
        "\n",
        "true_sequential: bool = True\n",
        "Forces strictly sequential quantization (layer by layer).\n",
        "Safer but slower.\n",
        "\n",
        "lm_head: bool = False\n",
        "Whether to quantize the final LM head (output projection).\n",
        "Usually skipped for better accuracy.\n",
        "\n",
        "quant_method: QUANT_METHOD = GPTQ\n",
        "Which quantization algorithm to use (GPTQ, GPTQv2, EoRA, QQQ, etc.).\n",
        "\n",
        "format: FORMAT = GPTQ\n",
        "Output format (GPTQ, Marlin, ExLlamaV2 kernels, etc.).\n",
        "\n",
        "mse: float = 0\n",
        "If set > 0, enables MSE minimization during quantization for extra accuracy recovery.\n",
        "\n",
        "parallel_packing: bool = True\n",
        "Enables multi-threaded packing of quantized weights for speedup.\n",
        "\n",
        "meta: Dict | None\n",
        "Extra metadata for custom configs.\n",
        "\n",
        "device: str | device | None\n",
        "Device to run quantization (cuda, cpu, mps, etc.).\n",
        "\n",
        "pack_dtype: str | dtype | None = torch.int32\n",
        "Packing dtype (int32 default, but can be int16 in some kernels).\n",
        "\n",
        "adapter: Dict | Lora | None\n",
        "Allows LoRA/EoRA adapters to be applied during or after quantization.\n",
        "\n",
        "rotation: str | None\n",
        "If rotation quantization (RQ, QRQ) is applied.\n",
        "\n",
        "is_marlin_format: bool = False\n",
        "If set, stores quantized weights in Marlin kernel format (fast inference).\n",
        "\n",
        "v2: bool = False\n",
        "Enables GPTQ v2 quantization (better accuracy, more VRAM required).\n",
        "\n",
        "v2_alpha: float = 0.25\n",
        "Extra parameter controlling Hessian approximation in GPTQv2.\n",
        "\n",
        "v2_memory_device: str = \"auto\"\n",
        "Controls where GPTQv2 Hessians are computed (cpu, gpu, or auto).\n",
        "\n",
        "mock_quantization: bool = False\n",
        "Runs a fake quantization pass (for debugging/testing without actually quantizing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNubqwJLrK3A"
      },
      "outputs": [],
      "source": [
        "# increase `batch_size` to match gpu/vram specs to speed up quantization\n",
        "model.quantize(calibration_dataset, batch_size=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIQ19rzbj8zJ"
      },
      "outputs": [],
      "source": [
        "model.save(quant_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B26AEbS6prsn"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn1Ki6kOsXLJ"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import whoami\n",
        "whoami()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMHMt4oUsZaz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import upload_folder\n",
        "upload_folder(\n",
        "    repo_id=\"sunny199/Llama-3.2-1B-Instruct-gptqmodel-4bit\",\n",
        "    folder_path=\"./Llama-3.2-1B-Instruct-gptqmodel-4bit\",\n",
        "    commit_message=\"Upload GPTQ quantized LLaMA-3.2-1B model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWRuzk4-uGPE"
      },
      "outputs": [],
      "source": [
        "# test post-quant inference\n",
        "model = GPTQModel.load(quant_path)\n",
        "result = model.generate(\"Uncovering deep insights begins with\")[0] # tokens\n",
        "print(model.tokenizer.decode(result)) # string output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV8Zmbw07rPi"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from gptqmodel import GPTQModel, QuantizeConfig\n",
        "import torch\n",
        "\n",
        "def calculate_avg_ppl(model, tokenizer, texts, max_length=512, batch_size=8):\n",
        "    \"\"\"Compute average perplexity over a list of plain text strings.\"\"\"\n",
        "    from gptqmodel.utils.perplexity import Perplexity\n",
        "\n",
        "    ppl = Perplexity(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        dataset_path=None,\n",
        "        split=None,\n",
        "        text_column=None,\n",
        "    )\n",
        "    # Pass list of text strings\n",
        "    return ppl.calculate_from_texts(texts, max_length, batch_size)\n",
        "\n",
        "def load_normal_model(model_id):\n",
        "    return GPTQModel.load(model_id, quantize_config=None)\n",
        "\n",
        "def load_quant_model(quant_path, device):\n",
        "    return GPTQModel.load(quant_path, device=device)\n",
        "\n",
        "def main():\n",
        "    model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "    quant_path = \"Llama-3.2-1B-Instruct-gptq-4bit\"\n",
        "    # Load calibration / eval dataset texts\n",
        "    calibration_texts = load_dataset(\n",
        "        \"allenai/c4\",\n",
        "        data_files=\"en/c4-train.00001-of-01024.json.gz\",\n",
        "        split=\"train\"\n",
        "    ).select(range(1024))[\"text\"]\n",
        "\n",
        "    # Subset for evaluation\n",
        "    eval_texts = calibration_texts[:100]\n",
        "\n",
        "    # Try loading quantized model if exists\n",
        "    try:\n",
        "        device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        quant_model = load_quant_model(quant_path, device=device)\n",
        "        quant_exists = True\n",
        "    except Exception as e:\n",
        "        print(\"Quantized model not found or failed to load:\", e)\n",
        "        quant_exists = False\n",
        "\n",
        "    # Load normal (unquantized) model\n",
        "    normal_model = load_normal_model(model_id)\n",
        "\n",
        "    # If quantized model doesn't exist, create it\n",
        "    if not quant_exists:\n",
        "        quant_config = QuantizeConfig(bits=4, group_size=128)\n",
        "        print(\"Quantizing model now...\")\n",
        "        model = GPTQModel.load(model_id, quant_config)\n",
        "        model.quantize(calibration_texts, batch_size=1)\n",
        "        model.save(quant_path)\n",
        "        quant_model = load_quant_model(quant_path, device=device)\n",
        "\n",
        "    # Evaluate normal model\n",
        "    print(\"Evaluating normal model...\")\n",
        "    ppl_normal = calculate_avg_ppl(normal_model, normal_model.tokenizer, eval_texts)\n",
        "    print(\"Normal model avg PPL:\", ppl_normal)\n",
        "\n",
        "    # Evaluate quantized model\n",
        "    print(\"Evaluating quantized model...\")\n",
        "    ppl_quant = calculate_avg_ppl(quant_model, quant_model.tokenizer, eval_texts)\n",
        "    print(\"Quantized model avg PPL:\", ppl_quant)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ-Aqw1MaDIl"
      },
      "source": [
        "Allocated = currently in use.\n",
        "\n",
        "Reserved = currently held by PyTorch (in use + cached).\n",
        "\n",
        "Peak Allocated = highest ever “in use.”\n",
        "\n",
        "Peak Reserved = highest ever “held by cache.”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hJfT-n4-ln1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def measure_gpu_memory(model_loader_fn, *args, **kwargs):\n",
        "    # Reset stats\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model_loader_fn(*args, **kwargs).to(device)\n",
        "\n",
        "    # Do a dummy forward pass to trigger memory usage (adjust input shape as per your model)\n",
        "    # For example, if it’s a language model:\n",
        "    dummy_input = torch.randint(0, 100, (1, 16)).to(device)\n",
        "    try:\n",
        "        _ = model.generate(dummy_input)\n",
        "    except Exception:\n",
        "        # fallback: try forward if generate not available\n",
        "        _ = model(dummy_input)\n",
        "\n",
        "    # Collect memory stats\n",
        "    mem_alloc = torch.cuda.memory_allocated(device)\n",
        "    mem_reserved = torch.cuda.memory_reserved(device)\n",
        "    peak_alloc = torch.cuda.max_memory_allocated(device)\n",
        "    peak_reserved = torch.cuda.max_memory_reserved(device)\n",
        "\n",
        "    print(f\"Memory allocated: {mem_alloc / (1024**2):.2f} MB\")\n",
        "    print(f\"Memory reserved:  {mem_reserved / (1024**2):.2f} MB\")\n",
        "    print(f\"Peak alloc:       {peak_alloc / (1024**2):.2f} MB\")\n",
        "    print(f\"Peak reserved:    {peak_reserved / (1024**2):.2f} MB\")\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def load_normal_model(model_id):\n",
        "    quantize_config=QuantizeConfig(bits=4, group_size=128)\n",
        "    return GPTQModel.load(model_id, quantize_config=quantize_config)\n",
        "#Example usage:\n",
        "measure_gpu_memory(lambda: load_normal_model(model_id))\n",
        "# measure_gpu_memory(lambda: load_quant_model(quant_path, device=\"cuda:0\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ed_oWVdukce"
      },
      "outputs": [],
      "source": [
        "# gptqmodel is integrated into lm-eval >= v0.4.7\n",
        "!pip install lm-eval>=0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBdZTXo1vt92"
      },
      "outputs": [],
      "source": [
        "from gptqmodel import GPTQModel\n",
        "from gptqmodel.utils.eval import EVAL\n",
        "\n",
        "model_id = \"ModelCloud/Llama-3.2-1B-Instruct-gptqmodel-4bit-vortex-v1\"\n",
        "\n",
        "# Use `lm-eval` as framework to evaluate the model\n",
        "lm_eval_results = GPTQModel.eval(model_id, framework=EVAL.LM_EVAL, tasks=[EVAL.LM_EVAL.ARC_CHALLENGE])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNIw9DHE1wNc"
      },
      "source": [
        "These are the evaluation metrics for the quantized model on the ARC Challenge task.\n",
        "\n",
        "acc is accuracy, acc_norm is normalized accuracy, etc.\n",
        "\n",
        "0.2799 means ~27.99% accuracy, with a standard error ±0.0131.\n",
        "\n",
        "The evaluation harness printed the result table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZLzxJLR2rqL"
      },
      "source": [
        "### Bits & Bytes / Quantization Context\n",
        "\n",
        "bitsandbytes is a library that implements efficient quantized operations (4-bit, 8-bit) for matrices / linear layers, especially useful for large models.\n",
        "\n",
        "The transformers library supports integration with bitsandbytes via a BitsAndBytesConfig class. That config tells from_pretrained how to load model weights in lower precision (4-bit or 8-bit) rather than default high precision.\n",
        "\n",
        "This integration is meant for inference (or adapter training) rather than full training from scratch. It reduces VRAM / memory footprint a lot, and you can still generate text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUUfFtFcsoCS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate\n",
        "!pip install -U bitsandbytese\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"   # example HF model\n",
        "bnb = BitsAndBytesConfig(load_in_4bit=True)      # or load_in_8bit=True\n",
        "tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, quantization_config=bnb, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "inputs = tok(\"Explain GPTQ vs AWQ simply.\", return_tensors=\"pt\").to(model.device)\n",
        "out = model.generate(**inputs, max_new_tokens=80)\n",
        "print(tok.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQRA1anw2WJJ"
      },
      "source": [
        "### ExLlama\n",
        "\n",
        "ExLlama is a Python/C++/CUDA implementation tailored for Llama (and Llama-style) models, working especially with 4-bit GPTQ weights.\n",
        "\n",
        "It provides optimized kernels (linear layers, quantized operators) for inference, to accelerate generation speed and reduce memory overhead.\n",
        "\n",
        "ExLlamaV2 adds more features: e.g. EXL2 format (a flexible quantization format), better kernel optimizations, dynamic batching, new generation strategies.\n",
        "\n",
        "ExLlama is often integrated into quantization+inference pipelines — e.g. Transformers’ GPTQ support may allow using ExLlama kernels by specifying certain config flags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9pnS9kiswif"
      },
      "outputs": [],
      "source": [
        "!pip install exllamav2\n",
        "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer, ExLlamaV2Generator\n",
        "from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Tokenizer, ExLlamaV2Generator\n",
        "# Download a GPTQ/EXL2 model first or point to HF snapshot dir with config + shards\n",
        "# Example assumes local path `./model` with GPTQ weights (ex: TheBloke GPTQ)\n",
        "model_path = \"/content/model\"  # put your GPTQ model dir here\n",
        "\n",
        "cfg = ExLlamaV2Config()\n",
        "cfg.model_dir = model_path\n",
        "model = ExLlamaV2(cfg)\n",
        "tok = ExLlamaV2Tokenizer(model_path)\n",
        "gen = ExLlamaV2Generator(model, tok)\n",
        "\n",
        "print(gen.generate_simple(\"Explain GPTQ vs AWQ simply.\", max_new_tokens=80))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}