{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu7PODo8C-9S"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U autoawq transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlLrUZZCqlm0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import gc ### garbaige collector\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTDTXEqcqtZz"
      },
      "outputs": [],
      "source": [
        "# Configure environment\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "base_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "print(\"Checking PyTorch and CUDA versions...\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfX59M6uPfc"
      },
      "outputs": [],
      "source": [
        "# Clear any existing cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNwuBoqbuS2d"
      },
      "outputs": [],
      "source": [
        "print(\"Loading tokenizer...\")\n",
        "tok = AutoTokenizer.from_pretrained(\n",
        "    base_model,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHPfLaMpuXkV"
      },
      "outputs": [],
      "source": [
        "print(\"Loading model...\")\n",
        "# Load model with specific configurations to avoid attention issues\n",
        "mdl = AutoAWQForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    low_cpu_mem_usage=True,\n",
        "    use_cache=False,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,  # Use fp16 explicitly\n",
        "    device_map={\"\": 0} if torch.cuda.is_available() else \"cpu\"  # More specific device mapping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRNyvLNeIF20"
      },
      "outputs": [],
      "source": [
        "# Set model to eval mode\n",
        "mdl.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZGU4NTSu5GQ"
      },
      "outputs": [],
      "source": [
        "# AWQ quantization config\n",
        "quant_config = {\n",
        "    \"zero_point\": True,\n",
        "    \"q_group_size\": 128,\n",
        "    \"w_bit\": 4,\n",
        "    \"version\": \"GEMM\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7wpOIX2u8z3"
      },
      "outputs": [],
      "source": [
        "# Create more diverse and shorter calibration data\n",
        "print(\"Preparing calibration data...\")\n",
        "calib_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning models process data efficiently.\",\n",
        "    \"Natural language understanding is advancing rapidly.\",\n",
        "    \"Deep neural networks learn complex patterns.\",\n",
        "    \"Artificial intelligence transforms technology.\",\n",
        "    \"Computer vision recognizes objects accurately.\",\n",
        "    \"Robotics integrates sensors and actuators.\",\n",
        "    \"Algorithm optimization improves performance significantly.\",\n",
        "    \"Data science extracts meaningful insights.\",\n",
        "    \"Software engineering creates reliable systems.\"\n",
        "] * 10  # 100 samples total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMh4r_mEKetx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_USE_SDPA\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDqrd-F_KlIN"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "cfg = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
        "cfg.attn_implementation = \"eager\"   # fallback attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZYnt6RZKqPg"
      },
      "outputs": [],
      "source": [
        "calib_tokens = [\n",
        "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
        "    for text in calib_texts[:50]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MIV2MMdKt6M"
      },
      "outputs": [],
      "source": [
        "mdl.quantize(\n",
        "    tok,\n",
        "    quant_config=quant_config,\n",
        "    calib_data=calib_tokens,\n",
        "    max_calib_seq_len=128,\n",
        "    max_calib_samples=50,\n",
        "    n_parallel_calib_samples=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LCM9JzOK3ow"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_USE_SDPA\"] = \"0\"   # Disable SDPA attention\n",
        "\n",
        "calib_tokens = [\n",
        "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids\n",
        "    for text in calib_texts[:50]\n",
        "]\n",
        "\n",
        "mdl.quantize(\n",
        "    tok,\n",
        "    quant_config=quant_config,\n",
        "    calib_data=calib_tokens,\n",
        "    max_calib_seq_len=128,\n",
        "    max_calib_samples=50,\n",
        "    n_parallel_calib_samples=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mCOWzxzLJSs"
      },
      "outputs": [],
      "source": [
        "calib_tokens = [\n",
        "    tok(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128).input_ids[0].tolist()\n",
        "    for text in calib_texts[:50]\n",
        "]\n",
        "\n",
        "mdl.quantize(\n",
        "    tok,\n",
        "    quant_config=quant_config,\n",
        "    calib_data=calib_tokens,   # now proper format\n",
        "    max_calib_seq_len=128,\n",
        "    max_calib_samples=50,\n",
        "    n_parallel_calib_samples=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfkDLulQLL4m"
      },
      "outputs": [],
      "source": [
        "mdl.quantize(\n",
        "    tok,\n",
        "    quant_config=quant_config,\n",
        "    calib_data=calib_texts[:50],   # raw text list\n",
        "    max_calib_seq_len=128,\n",
        "    max_calib_samples=50,\n",
        "    n_parallel_calib_samples=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FutaFE1xIOP5"
      },
      "outputs": [],
      "source": [
        "# Save the quantized model\n",
        "out_dir = \"tinyllama-1.1b-awq\"\n",
        "print(f\"Saving model to {out_dir}...\")\n",
        "\n",
        "mdl.save_quantized(out_dir, safetensors=True)\n",
        "tok.save_pretrained(out_dir)\n",
        "\n",
        "# Save config\n",
        "config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
        "config.save_pretrained(out_dir)\n",
        "\n",
        "print(f\"Model successfully quantized and saved to {out_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qy-hTtLNc6Wh"
      },
      "outputs": [],
      "source": [
        "print(\"Starting quantization...\")\n",
        "try:\n",
        "    # Use minimal configuration to avoid batch size issues\n",
        "    mdl.quantize(\n",
        "        tok,\n",
        "        quant_config=quant_config,\n",
        "        calib_data=calib_texts,\n",
        "        max_calib_seq_len=128,     # Reduced sequence length\n",
        "        max_calib_samples=50,      # Reduced sample count\n",
        "        n_parallel_calib_samples=1 # Keep sequential processing\n",
        "    )\n",
        "\n",
        "    print(\"Quantization completed successfully!\")\n",
        "\n",
        "    # Save the quantized model\n",
        "    out_dir = \"tinyllama-1.1b-awq\"\n",
        "    print(f\"Saving model to {out_dir}...\")\n",
        "\n",
        "    mdl.save_quantized(out_dir, safetensors=True)\n",
        "    tok.save_pretrained(out_dir)\n",
        "\n",
        "    # Save config\n",
        "    config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
        "    config.save_pretrained(out_dir)\n",
        "\n",
        "    print(f\"Model successfully quantized and saved to {out_dir}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed with error: {str(e)}\")\n",
        "    print(\"Trying alternative approach...\")\n",
        "    print(\"\\nTrying alternative approach with different model loading...\")\n",
        "\n",
        "    # Alternative approach: Load model differently\n",
        "    del mdl\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    try:\n",
        "        # Try loading without device_map first\n",
        "        mdl = AutoAWQForCausalLM.from_pretrained(\n",
        "            base_model,\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True,\n",
        "            use_cache=False\n",
        "        )\n",
        "\n",
        "        # Move to device manually if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            mdl = mdl.cuda()\n",
        "\n",
        "        mdl.eval()\n",
        "\n",
        "        # Try with even smaller calibration parameters\n",
        "        print(\"Attempting quantization with minimal parameters...\")\n",
        "        mdl.quantize(\n",
        "            tok,\n",
        "            quant_config=quant_config,\n",
        "            calib_data=calib_texts[:20],  # Only use first 20 samples\n",
        "            max_calib_seq_len=64,         # Even smaller sequence length\n",
        "            max_calib_samples=20,         # Minimal samples\n",
        "            n_parallel_calib_samples=1\n",
        "        )\n",
        "\n",
        "        out_dir = \"tinyllama-1.1b-awq\"\n",
        "        mdl.save_quantized(out_dir, safetensors=True)\n",
        "        tok.save_pretrained(out_dir)\n",
        "\n",
        "        config = AutoConfig.from_pretrained(base_model, trust_remote_code=True)\n",
        "        config.save_pretrained(out_dir)\n",
        "\n",
        "        print(f\"Model successfully quantized with alternative approach and saved to {out_dir}\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"Alternative approach also failed: {str(e2)}\")\n",
        "        print(\"\\nConsider using a pre-quantized model instead:\")\n",
        "        print(\"TheBloke/TinyLlama-1.1B-Chat-v1.0-AWQ\")\n",
        "\n",
        "finally:\n",
        "    # Cleanup\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sRw8NNXLqA5"
      },
      "source": [
        "The script runs an AWQ quantization pipeline with a safe, low-memory fallback path.\n",
        "\n",
        "Primary flow:\n",
        "\n",
        "Calls mdl.quantize(...) using your tokenizer, AWQ quant config, and calibration texts (prompts).\n",
        "\n",
        "Uses conservative settings to avoid OOM: max_calib_seq_len=128, max_calib_samples=50, n_parallel_calib_samples=1.\n",
        "\n",
        "On success, saves the quantized weights, tokenizer, and model config to out_dir.\n",
        "\n",
        "If quantization fails:\n",
        "\n",
        "Frees memory (del, torch.cuda.empty_cache(), gc.collect()).\n",
        "\n",
        "Reloads the model with low_cpu_mem_usage=True and use_cache=False (and moves to GPU if available).\n",
        "\n",
        "Retries quantization with smaller calibration settings (shorter seq length, fewer samples).\n",
        "\n",
        "If it still fails, suggests using a pre-quantized AWQ model.\n",
        "\n",
        "Role of prompts (calibration texts):\n",
        "\n",
        "They provide short, representative inputs so AWQ can observe activations and pick good weight scales.\n",
        "\n",
        "More and more realistic prompts ⇒ better quality after quantization; your code uses a small set to keep memory low.\n",
        "\n",
        "Important parameters:\n",
        "\n",
        "max_calib_seq_len & max_calib_samples: control calibration length/size (quality vs. memory).\n",
        "\n",
        "n_parallel_calib_samples: set to 1 for minimal VRAM use.\n",
        "\n",
        "low_cpu_mem_usage=True, use_cache=False: reduce RAM/GPU memory footprint during load/quant.\n",
        "\n",
        "safetensors=True, trust_remote_code=True: safer format and allow custom model code.\n",
        "\n",
        "Gotchas / prerequisites:\n",
        "\n",
        "Ensure variables & imports exist: mdl, tok, calib_texts, quant_config, base_model, plus torch, gc, AutoConfig, and AutoAWQForCausalLM (from the AWQ library).\n",
        "\n",
        "Don’t mix GPTQ configs with AWQ; use the right quant config for the backend.\n",
        "\n",
        "If you hit OOM, reduce max_calib_seq_len/samples further or increase group size.\n",
        "\n",
        "After quantization:\n",
        "\n",
        "Load the saved model directory with the AWQ loader (from_quantized) and run generation as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqiL6VH8c6wC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}