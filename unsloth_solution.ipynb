{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9H5kQY0Uac5"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu128\n",
        "!pip install unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth  # MUST BE FIRST\n",
        "\n",
        "import time, torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "nEK0vSutWnu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert torch.cuda.is_available()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def peak_vram():\n",
        "    return round(torch.cuda.max_memory_reserved()/1024**3, 3)\n",
        "\n",
        "def now():\n",
        "    return time.time()"
      ],
      "metadata": {
        "id": "qQmSvZ3HWvNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def prepare_dataset(n=200):\n",
        "    ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\").select(range(n))\n",
        "    def fmt(ex):\n",
        "        return {\"text\": alpaca_prompt.format(\n",
        "            instruction=ex[\"instruction\"],\n",
        "            input=ex[\"input\"]\n",
        "        ) + ex[\"output\"]}\n",
        "    return ds.map(fmt, remove_columns=ds.column_names)\n",
        "\n",
        "dataset = prepare_dataset()"
      ],
      "metadata": {
        "id": "J4GPlPfYW3L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = now()\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
        "    max_seq_length=1024,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "XQrkSBU9W6Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    args=SFTConfig(\n",
        "        max_steps=50,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        output_dir=\"unsloth_out\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "train_time = round(now() - start, 2)\n",
        "train_vram = peak_vram()\n"
      ],
      "metadata": {
        "id": "Yfg6PW-jW9te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(\"Explain LoRA simply.\", return_tensors=\"pt\").to(\"cuda\")\n",
        "torch.cuda.synchronize()\n",
        "t0 = now()\n",
        "out = model.generate(**inputs, max_new_tokens=128)\n",
        "torch.cuda.synchronize()\n",
        "t1 = now()\n",
        "\n",
        "tokens_per_sec = round(out.shape[-1]/(t1 - t0), 2)\n",
        "\n",
        "print(\"UNSLOTH RESULTS\")\n",
        "print(\"Train time (sec):\", train_time)\n",
        "print(\"Peak VRAM (GB):\", train_vram)\n",
        "print(\"Tokens/sec:\", tokens_per_sec)\n"
      ],
      "metadata": {
        "id": "MVdN18g3W_z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Same dataset, same steps, same batch size.\n",
        "Only engine changed — Hugging Face vs Unsloth.”"
      ],
      "metadata": {
        "id": "f7h4Cx4yXX_2"
      }
    }
  ]
}