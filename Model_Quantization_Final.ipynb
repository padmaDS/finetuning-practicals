{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### PTQ (Post-Training Quantization)\n",
        "\n",
        "### PTQ Dynamic\n",
        "\n",
        "\n",
        "#### Manual weight quantization AFTER training, without retraining, and without any calibration.\n",
        "\n",
        "Weights: Quantized manually (INT8 → dequantized and copied into new model)\n",
        "\n",
        "Activations: Not touched at all — still in FP32\n",
        "\n",
        "Calibration: didn't perform\n",
        "\n",
        "No retraining / QAT\n",
        "\n",
        "This is a manual implementation of Dynamic Post-Training Quantization where only the weights are quantized and copied to a new model after training.\n",
        "\n",
        "So this is Closest to:\n",
        "\n",
        "Dynamic PTQ(manual implementation of Dynamic Post-Training Quantization)\n",
        "\n",
        "✅ Weights quantized\n",
        "\n",
        "❌ Activations untouched\n",
        "\n",
        "❌ No calibration\n",
        "\n",
        "❌ No QAT\n"
      ],
      "metadata": {
        "id": "bQyd0nMqPm5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "8lGtrAXbXb7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_moons(n_samples=5000, noise=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "yqSTRfYpO4Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hukJ1h3nIiDq",
        "outputId": "456d3ad8-2ea8-4784-b12f-5700187fff84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.64527536,  1.38251014],\n",
              "       [ 0.14514823, -0.32157033],\n",
              "       [ 0.11945131,  0.41631146],\n",
              "       ...,\n",
              "       [ 0.6360473 ,  0.66530771],\n",
              "       [ 1.61542641, -0.24249711],\n",
              "       [ 0.10599548,  1.0899585 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCapbER4sv-A",
        "outputId": "99ece562-e16a-470c-f5f2-f300696f05e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr')\n",
        "plt.title(\"make_moons Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "I4omVFzsICEh",
        "outputId": "d93a69a3-84d3-4c14-e5ae-f141f454d24c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = StandardScaler().fit_transform(X)  # normalize"
      ],
      "metadata": {
        "id": "01xYPM4gO8Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "H0hPoNwrIM7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "07PDcouzO_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VgadFH_dO_76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Original FP32 Model\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(2, 16)\n",
        "#         self.fc2 = nn.Linear(16, 8)\n",
        "#         self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         return torch.sigmoid(self.fc3(x))"
      ],
      "metadata": {
        "id": "pJj-6421vrvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BigMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 16)\n",
        "        self.fc6 = nn.Linear(16, 8)\n",
        "        self.fc7 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        x = F.relu(self.fc6(x))\n",
        "        return torch.sigmoid(self.fc7(x))"
      ],
      "metadata": {
        "id": "iqCq3BLpPCcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32 = BigMLP()"
      ],
      "metadata": {
        "id": "rZWAWGUVPEKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam = Adaptive Moment Estimation\n",
        "\n",
        "torch.optim.Adam() is what actually helps your neural network \"learn\" by updating its internal weights every step, using gradients and momentum.\n",
        "\n",
        "model_fp32.parameters()\n",
        "\n",
        "This means:\n",
        "\n",
        "\"Give the optimizer all the learnable parameters of the model —\n",
        "which includes its weights and biases — so that the optimizer can update them during training.\"\n",
        "\n",
        "lr=0.01\n",
        "\n",
        "This is the learning rate, which controls:\n",
        "\n",
        "\"How fast the model updates its weights in each training step.\"\n",
        "\n",
        "A higher learning rate → faster learning, but there's a risk of overshooting the optimal point.\n",
        "\n",
        "A lower learning rate → slower and more stable learning, but it takes longer to converge."
      ],
      "metadata": {
        "id": "LQ_zXwIqMZgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYSTLFRINH3x",
        "outputId": "1bf9745f-d520-4256-9a27-fdc9894c9032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7c080817a960>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_fp32.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "BqAoS1DQPGeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "I9Y93Lv8PHlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2000):\n",
        "    model_fp32.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model_fp32(X_train)\n",
        "\n",
        "    loss = loss_fn(out, y_train)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAJrW5A-PJVy",
        "outputId": "ff3029d2-000f-4397-bd82-cd0dfefaa101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.7018\n",
            "Epoch 500 | Loss: 0.0097\n",
            "Epoch 1000 | Loss: 0.0171\n",
            "Epoch 1500 | Loss: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is thresholding the predictions:\n",
        "\n",
        "If prediction > 0.5 → output = 1\n",
        "\n",
        "Else → output = 0\n",
        "\n",
        "Then converts the Boolean tensor into float (i.e., 1.0 or 0.0)\n",
        "\n",
        "Basically:\n",
        "\n",
        "preds = [0.91, 0.23, 0.66] → ❌ not usable\n",
        "\n",
        "So we convert:\n",
        "\n",
        "preds = [1.0, 0.0, 1.0]"
      ],
      "metadata": {
        "id": "4D5zWcaXNt1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compares the predictions with the actual labels (y)\n",
        "\n",
        "(preds == y) → gives a tensor of True/False → cast to float → 1.0 or 0.0\n",
        "\n",
        ".mean() → average of all 1s and 0s = accuracy\n",
        "\n",
        ".item() → convert final PyTorch scalar to a regular Python float (like 0.87)\n",
        "\n"
      ],
      "metadata": {
        "id": "TbrYQvDFNqyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        preds = model(X)\n",
        "\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "        return (preds == y).float().mean().item()"
      ],
      "metadata": {
        "id": "YBFdenY2PMgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFP32 Accuracy:\", accuracy(model_fp32, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dv2RSijVPQRq",
        "outputId": "b43342dc-c262-487f-89a7-9b38e1bb0c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP32 Accuracy: 0.9700000286102295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.quantization import quantize_dynamic"
      ],
      "metadata": {
        "id": "GJHmh8S5XNoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REAL DYNAMIC QUANTIZATION\n",
        "model_int8 = quantize_dynamic(\n",
        "    model_fp32,             # Original FP32 model\n",
        "    {nn.Linear},            # Which layers to quantize\n",
        "    dtype=torch.qint8       # Quantization dtype\n",
        ")"
      ],
      "metadata": {
        "id": "6oEcRnFWqfwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-coPgmcXg24",
        "outputId": "4ae09830-cc24-4b43-b7d7-17f88688ce31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigMLP(\n",
              "  (fc1): DynamicQuantizedLinear(in_features=2, out_features=128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc2): DynamicQuantizedLinear(in_features=128, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc3): DynamicQuantizedLinear(in_features=64, out_features=64, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc4): DynamicQuantizedLinear(in_features=64, out_features=32, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc5): DynamicQuantizedLinear(in_features=32, out_features=16, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc6): DynamicQuantizedLinear(in_features=16, out_features=8, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              "  (fc7): DynamicQuantizedLinear(in_features=8, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"INT8 Quantized Accuracy:\", accuracy(model_int8, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XlArEFWqh6f",
        "outputId": "302db848-3a69-4d43-822b-f632ac15a560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INT8 Quantized Accuracy: 0.9700000286102295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fp32.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3b-Ug8XX9iv",
        "outputId": "028519c1-5009-4e97-e001-fd82223ed89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[-0.0327, -0.3705],\n",
              "                      [-0.7268, -0.3635],\n",
              "                      [-0.0281, -0.1552],\n",
              "                      [ 0.0126,  0.9720],\n",
              "                      [-0.4855, -0.0056],\n",
              "                      [ 0.9236, -0.1011],\n",
              "                      [-0.1847, -0.1562],\n",
              "                      [ 0.9931,  0.3162],\n",
              "                      [ 0.6974, -0.3151],\n",
              "                      [ 0.3658, -0.2927],\n",
              "                      [ 0.0530, -0.4962],\n",
              "                      [ 0.7572, -0.2966],\n",
              "                      [-0.2148, -0.2911],\n",
              "                      [ 1.0573, -0.4012],\n",
              "                      [-0.3342,  0.1330],\n",
              "                      [ 0.3522, -0.0953],\n",
              "                      [ 0.0115, -0.5237],\n",
              "                      [ 0.3698,  0.5628],\n",
              "                      [-0.1818,  0.7531],\n",
              "                      [ 0.1182,  0.5494],\n",
              "                      [-0.7885, -0.1293],\n",
              "                      [ 0.5033, -0.5177],\n",
              "                      [-0.3387, -0.3617],\n",
              "                      [ 0.4590,  0.9177],\n",
              "                      [-0.2369,  0.0045],\n",
              "                      [-0.6962, -0.1326],\n",
              "                      [ 0.3080,  0.3988],\n",
              "                      [ 0.9364,  0.4888],\n",
              "                      [ 0.0234, -0.0951],\n",
              "                      [-0.0715,  0.0339],\n",
              "                      [ 0.8618, -0.0166],\n",
              "                      [ 0.5109, -0.3663],\n",
              "                      [-0.6361,  0.1933],\n",
              "                      [-0.7662,  0.0409],\n",
              "                      [ 0.3950, -0.9633],\n",
              "                      [ 0.3073, -0.0741],\n",
              "                      [ 0.3821,  0.6205],\n",
              "                      [ 0.5072,  0.0525],\n",
              "                      [-0.7393, -0.2807],\n",
              "                      [-0.4806, -0.1235],\n",
              "                      [-0.6387,  0.1108],\n",
              "                      [ 0.0265, -0.2077],\n",
              "                      [-0.7977,  0.4980],\n",
              "                      [ 0.5765,  0.5060],\n",
              "                      [-0.0636, -0.0596],\n",
              "                      [ 0.1118, -0.2522],\n",
              "                      [ 0.4860,  0.0186],\n",
              "                      [-0.4092,  0.0240],\n",
              "                      [ 0.2083, -0.0230],\n",
              "                      [ 0.5101, -0.2862],\n",
              "                      [ 0.4231,  0.6069],\n",
              "                      [ 0.6727, -0.0080],\n",
              "                      [-0.7379, -0.3491],\n",
              "                      [-0.5801, -0.5484],\n",
              "                      [-0.5574,  0.0025],\n",
              "                      [ 0.5518, -0.5716],\n",
              "                      [-0.7421, -0.0977],\n",
              "                      [ 0.2670, -0.8268],\n",
              "                      [ 0.8526, -0.0957],\n",
              "                      [ 0.3810,  0.3096],\n",
              "                      [ 0.3221,  0.5079],\n",
              "                      [ 0.8732, -0.0218],\n",
              "                      [-0.7960, -0.1189],\n",
              "                      [ 0.3021,  0.0514],\n",
              "                      [ 0.6722, -0.0299],\n",
              "                      [ 0.7590, -0.0294],\n",
              "                      [ 1.2469,  0.0064],\n",
              "                      [-0.1753,  0.0025],\n",
              "                      [-0.9945,  0.0494],\n",
              "                      [ 0.3599, -0.5379],\n",
              "                      [-0.0539, -0.5325],\n",
              "                      [-0.4534,  0.7942],\n",
              "                      [-0.1914,  0.1789],\n",
              "                      [ 1.0416,  0.0621],\n",
              "                      [ 0.4815, -0.3488],\n",
              "                      [-0.5827,  0.1796],\n",
              "                      [-0.4221, -0.0938],\n",
              "                      [ 0.8956, -0.0700],\n",
              "                      [-0.1106,  0.9759],\n",
              "                      [-0.6697, -0.6773],\n",
              "                      [-0.1810,  0.1058],\n",
              "                      [-0.3003,  0.4892],\n",
              "                      [-0.2722, -0.6908],\n",
              "                      [ 0.0880, -0.3797],\n",
              "                      [-0.8337, -0.8322],\n",
              "                      [ 0.3065, -0.0530],\n",
              "                      [ 0.2429,  0.0545],\n",
              "                      [ 0.0237,  0.0189],\n",
              "                      [-0.9344, -0.0698],\n",
              "                      [ 0.2307, -0.1309],\n",
              "                      [ 0.3059, -0.2376],\n",
              "                      [-0.0555, -0.6139],\n",
              "                      [ 1.0957,  0.0148],\n",
              "                      [ 0.2271, -0.0972],\n",
              "                      [ 1.1902, -0.4514],\n",
              "                      [-0.8763,  0.0091],\n",
              "                      [ 0.4604, -0.7258],\n",
              "                      [ 0.2697, -0.1639],\n",
              "                      [ 0.2410,  0.2136],\n",
              "                      [-0.9752, -0.0862],\n",
              "                      [ 0.0794, -0.8737],\n",
              "                      [ 0.8596, -0.1873],\n",
              "                      [-0.0265,  0.2755],\n",
              "                      [-0.1181, -1.0543],\n",
              "                      [ 0.4734, -0.0202],\n",
              "                      [-0.9168, -0.6877],\n",
              "                      [ 0.2400, -0.0766],\n",
              "                      [-0.5590, -0.1605],\n",
              "                      [ 0.1087, -0.7658],\n",
              "                      [ 0.4938,  0.9768],\n",
              "                      [ 0.6996,  0.5038],\n",
              "                      [ 0.1974,  0.0170],\n",
              "                      [ 1.0267, -0.0640],\n",
              "                      [ 0.2124, -0.1356],\n",
              "                      [-0.4862,  0.4741],\n",
              "                      [ 0.5849, -0.3374],\n",
              "                      [ 0.6287, -0.4368],\n",
              "                      [ 0.1639,  0.0780],\n",
              "                      [ 0.8772,  0.2032],\n",
              "                      [-0.3655, -0.8843],\n",
              "                      [ 0.8282,  0.0922],\n",
              "                      [-0.0919,  0.3408],\n",
              "                      [ 0.5645, -0.6615],\n",
              "                      [ 0.8278, -0.4262],\n",
              "                      [ 1.5157,  0.8918],\n",
              "                      [-0.1690, -0.2252],\n",
              "                      [-0.8364,  0.4343],\n",
              "                      [ 0.0212,  0.8897]])),\n",
              "             ('fc1.bias',\n",
              "              tensor([-5.3108e-01,  2.4700e-01,  2.7344e-01, -3.7837e-01,  1.0288e-01,\n",
              "                       2.6775e-01, -1.0328e-01,  4.0630e-01, -2.0215e-01,  7.3900e-01,\n",
              "                       5.4134e-01, -2.2675e-01,  1.9400e-01,  3.7060e-02,  1.3986e-01,\n",
              "                      -3.8499e-02, -5.1175e-01, -6.4161e-01, -3.2293e-01,  1.7506e-02,\n",
              "                      -5.3674e-01, -4.0158e-01,  3.7544e-02, -3.5627e-01,  7.1253e-01,\n",
              "                      -4.6968e-01,  1.5091e-01,  3.2218e-01, -6.0395e-01, -5.6249e-01,\n",
              "                       6.6622e-04, -6.4277e-01,  1.8685e-01, -4.7357e-01,  4.3536e-01,\n",
              "                      -3.6302e-01, -6.8313e-01,  3.3249e-01, -2.1856e-01,  1.4165e-01,\n",
              "                      -5.3141e-01, -2.1486e-01, -5.4902e-01, -6.4705e-01,  8.2692e-01,\n",
              "                      -2.5899e-01,  4.1420e-01, -3.7899e-01,  5.2927e-01, -6.0321e-01,\n",
              "                      -6.7052e-01, -5.1138e-01,  1.3834e-02,  1.4084e-01, -4.1113e-01,\n",
              "                      -7.7193e-01, -6.1843e-01, -6.1855e-01, -4.7071e-01,  1.0223e-01,\n",
              "                      -5.9950e-01, -4.5070e-01, -6.3252e-01, -3.7765e-01, -5.3303e-01,\n",
              "                      -5.8132e-01,  3.5766e-01, -9.3785e-02, -6.9065e-01,  3.8343e-01,\n",
              "                       5.2188e-01, -5.4864e-01, -6.5857e-01,  2.4317e-01, -6.4902e-01,\n",
              "                      -4.8816e-01, -1.7525e-01, -4.8249e-01, -5.3374e-01,  3.1002e-01,\n",
              "                      -5.9564e-01, -9.0548e-02, -3.8007e-01, -3.7959e-01, -3.4180e-01,\n",
              "                       3.5942e-02, -2.6659e-01, -1.9208e-01, -5.5244e-01,  8.1199e-01,\n",
              "                      -1.4888e-01, -4.2803e-01, -5.6661e-01, -9.8942e-02,  4.6425e-01,\n",
              "                      -5.9655e-01, -5.5964e-01,  7.6118e-02,  2.4850e-01, -5.8190e-01,\n",
              "                      -4.9298e-01,  3.1473e-01,  1.4450e-01, -6.7127e-01,  2.1111e-01,\n",
              "                      -5.1001e-01, -2.0845e-01,  2.2201e-01, -5.0876e-01, -3.8717e-01,\n",
              "                      -7.4759e-01, -3.5764e-01,  2.9791e-01,  6.2587e-02, -4.8337e-01,\n",
              "                      -2.3480e-01, -7.0901e-01, -3.2022e-01,  4.7303e-01, -1.4940e-01,\n",
              "                      -5.7936e-01, -1.4785e-01, -8.3813e-01, -3.4240e-01, -1.3542e-01,\n",
              "                      -4.1672e-01, -4.7378e-01, -3.4631e-01])),\n",
              "             ('fc2.weight',\n",
              "              tensor([[ 0.3859, -0.0160, -0.0647,  ...,  0.0416, -0.2848, -0.3711],\n",
              "                      [-0.1434, -0.1467, -0.1903,  ...,  0.0625,  0.2483,  0.4743],\n",
              "                      [-0.0811, -0.1422, -0.0476,  ..., -0.0924, -0.1922, -0.2020],\n",
              "                      ...,\n",
              "                      [ 0.3830,  0.1782,  0.0296,  ...,  0.2370, -0.3374, -0.1712],\n",
              "                      [ 0.4361,  0.2460,  0.0136,  ..., -0.0141,  0.3335, -0.0548],\n",
              "                      [ 0.0527, -0.1329,  0.0084,  ...,  0.0839, -0.1099, -0.0970]])),\n",
              "             ('fc2.bias',\n",
              "              tensor([-0.0217, -0.1359, -0.1367, -0.0767, -0.1035, -0.2515,  0.0309,  0.0488,\n",
              "                      -0.1604, -0.1367, -0.1879, -0.1354, -0.1375, -0.1765, -0.1497, -0.0099,\n",
              "                      -0.1271, -0.0887, -0.1356, -0.0551, -0.0680,  0.1945, -0.1017, -0.2694,\n",
              "                      -0.2331, -0.1612, -0.0728,  0.0752, -0.0223, -0.2910, -0.0222, -0.0833,\n",
              "                      -0.1074, -0.1271, -0.2270, -0.1087, -0.0877, -0.0268, -0.1351, -0.2977,\n",
              "                      -0.0531, -0.0936, -0.1600, -0.1860, -0.1099, -0.1956, -0.1774, -0.0586,\n",
              "                      -0.1013, -0.0087, -0.2094, -0.1970, -0.0504, -0.2358, -0.2790,  0.1268,\n",
              "                      -0.1147,  0.1019, -0.0715, -0.0408, -0.1186,  0.0514, -0.2951, -0.0863])),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 0.0251, -0.1101,  0.0101,  ..., -0.0296,  0.1091,  0.0282],\n",
              "                      [-0.0112, -0.4422, -0.0984,  ...,  0.7529, -0.0239,  0.0602],\n",
              "                      [-0.8162, -0.4162, -0.0090,  ..., -1.3591, -0.8701, -0.0375],\n",
              "                      ...,\n",
              "                      [-0.1752,  0.3082,  0.0076,  ...,  0.2469,  0.9451,  0.0338],\n",
              "                      [-0.2929,  0.0450,  0.0491,  ...,  0.1956,  0.8867, -0.0574],\n",
              "                      [ 0.0806, -0.1074,  0.0603,  ..., -0.1100, -0.5142,  0.0129]])),\n",
              "             ('fc3.bias',\n",
              "              tensor([-2.0294e-01, -3.3308e-01,  2.4274e-01, -1.0133e-01, -1.7262e-01,\n",
              "                      -2.3086e-01, -4.8288e-02,  4.2867e-02,  3.2239e-01,  2.1096e-01,\n",
              "                      -9.3146e-02, -8.4195e-02, -2.2106e-01, -3.5766e-02,  2.8280e-01,\n",
              "                      -1.7843e-01, -2.0468e-01, -7.4813e-02, -1.3435e+00,  1.3522e-01,\n",
              "                      -9.1660e-02,  5.1628e-02, -1.5493e-01, -2.5197e-01, -3.4978e-01,\n",
              "                       2.0210e-01, -3.5688e-01, -1.8639e-01, -2.1030e-01, -2.8606e-01,\n",
              "                       8.2848e-02,  6.5852e-04,  5.6948e-02, -3.6549e-01, -1.6088e-01,\n",
              "                       1.7226e-02, -1.1607e-01, -4.5733e-01, -5.5511e-02, -3.8372e-02,\n",
              "                       2.6160e-01, -1.0440e-01, -5.9630e-02, -2.5984e-01, -2.1666e-01,\n",
              "                       3.6073e-02,  3.4878e-02, -1.6761e-01, -4.0203e-02,  8.9323e-01,\n",
              "                      -1.3037e-01,  2.5348e-02, -7.8261e-01, -3.3209e-01,  1.4006e-01,\n",
              "                       1.1700e-02, -9.1381e-02, -5.1082e-02, -4.1714e-01,  3.2196e-03,\n",
              "                      -8.0616e-02, -7.6607e-02, -1.2390e-01, -2.8499e-01])),\n",
              "             ('fc4.weight',\n",
              "              tensor([[ 0.0535, -0.0818,  1.3670,  ...,  0.2575,  0.3092,  0.0833],\n",
              "                      [ 0.0054, -0.1131, -0.0530,  ..., -0.0721,  0.0551, -0.1647],\n",
              "                      [-0.0763, -0.0583, -0.0643,  ..., -0.0328, -0.1058, -0.0514],\n",
              "                      ...,\n",
              "                      [-0.0548, -0.0491, -0.1205,  ...,  0.0477, -0.0916, -0.1808],\n",
              "                      [-0.0791,  0.1188, -0.0340,  ..., -0.1018, -0.0197,  0.0110],\n",
              "                      [ 0.0660, -0.0821,  0.0973,  ..., -0.2647, -0.2595, -0.0505]])),\n",
              "             ('fc4.bias',\n",
              "              tensor([-0.1386, -0.0154, -0.0905, -0.0393, -0.1491, -0.4994, -0.0891, -0.0587,\n",
              "                      -0.0233, -0.1456, -0.1120, -0.0341,  0.4917, -0.2749,  0.0120, -0.2743,\n",
              "                       0.3082, -0.2317,  0.0063,  0.0584, -0.7562, -0.1433, -0.1292, -0.1726,\n",
              "                      -0.1761,  0.4071, -0.1261, -0.1872, -0.0035, -0.0106, -0.2741, -0.2421])),\n",
              "             ('fc5.weight',\n",
              "              tensor([[-7.2684e-02,  6.6007e-02,  4.1169e-02, -1.1913e-01, -1.1869e-01,\n",
              "                        3.8355e-02,  7.3113e-02, -3.2429e-02, -4.6687e-01,  6.5923e-02,\n",
              "                        8.1850e-02, -1.4848e-01, -3.6298e-03, -6.4427e-02,  2.0830e-02,\n",
              "                       -2.6863e-02, -2.3303e-01, -6.2259e-02, -1.4582e-01, -1.0573e-01,\n",
              "                       -1.2754e-02,  7.9153e-02, -3.4287e-01,  6.8450e-03, -2.8916e-01,\n",
              "                       -2.0378e-01, -1.3095e-01, -2.4184e-01, -8.8252e-02,  5.4360e-02,\n",
              "                        5.8319e-03, -1.1083e-02],\n",
              "                      [ 2.0484e-01, -2.2919e-02, -7.6702e-02,  2.3472e-01, -5.2642e-02,\n",
              "                        1.7819e-01,  2.7024e-02, -1.5964e-01, -8.6666e-03, -1.1139e-02,\n",
              "                        2.1904e-01, -3.3006e+00, -4.6283e-02, -7.8166e-02,  2.2734e-01,\n",
              "                        3.1762e-01,  3.1454e-01,  2.3700e-01,  1.7030e-01, -1.2294e-01,\n",
              "                       -9.9014e-01,  5.2191e-02,  1.0691e-01, -5.9988e-04, -1.5878e-01,\n",
              "                        2.5931e-01,  1.7583e-01,  2.6255e-01,  4.0380e-02, -1.2099e-02,\n",
              "                       -2.7805e-02,  5.8831e-02],\n",
              "                      [-2.7980e-01,  8.3151e-02,  7.9017e-02, -3.4494e-01,  1.1401e-01,\n",
              "                       -1.0807e-01, -2.1685e-02, -4.6993e-01, -1.4998e-01,  1.7816e-01,\n",
              "                       -1.1970e-01, -9.3991e-02, -3.1196e-01,  1.0664e-01,  7.4430e-03,\n",
              "                       -1.2929e-01, -8.9344e-02, -1.5803e-01, -2.6378e-01,  9.3814e-02,\n",
              "                        7.9690e-01, -1.5332e-02, -1.7517e-01, -6.8046e-02, -2.5127e-01,\n",
              "                       -4.5170e-01, -3.6565e-01, -2.2669e-02,  3.1423e-02, -1.7752e-01,\n",
              "                        1.3133e-01, -1.4032e-01],\n",
              "                      [-1.4691e-01, -1.6240e-01, -9.4170e-02, -2.1574e-01,  5.8594e-02,\n",
              "                       -2.9754e-01, -2.0117e-01, -1.9515e-01, -2.4582e-01, -1.2353e-01,\n",
              "                        1.7444e-01, -6.2260e-02, -3.5777e-01, -1.7923e-01, -2.3403e-01,\n",
              "                       -3.8249e-01, -2.4599e-01, -1.9509e-01, -2.2795e-01,  2.7364e-02,\n",
              "                        8.9787e-02, -1.3416e-01, -4.0443e-01,  1.2501e-01, -1.0278e-01,\n",
              "                       -4.4728e-01, -1.4311e-01, -3.7573e-01, -6.3928e-02,  1.3487e-01,\n",
              "                        6.1734e-01, -2.8242e-02],\n",
              "                      [-1.3750e-01, -1.0760e-02, -9.2379e-02, -2.2007e-01, -2.2402e-01,\n",
              "                       -1.9881e-01, -6.9151e-02, -7.7225e-02, -1.2754e-01, -9.2559e-02,\n",
              "                       -4.1720e-02,  7.6684e-03, -1.3237e-01,  1.2082e-01,  2.0309e-02,\n",
              "                       -6.4466e-02, -8.3204e-02, -1.3710e-02, -1.6078e-01,  1.0026e-01,\n",
              "                        1.4410e+00,  1.2287e-01, -8.0834e-03,  1.8551e-01, -1.5949e-01,\n",
              "                       -4.4102e-03, -4.5674e-02, -7.5998e-02,  2.8762e-04, -2.2504e-01,\n",
              "                       -1.6713e-01, -2.3025e-01],\n",
              "                      [-5.3813e-02,  1.4802e-01, -3.0588e-02, -4.8979e-02, -7.5158e-02,\n",
              "                       -3.5475e-01, -8.3563e-03, -1.7309e-02, -3.1658e-01,  6.4823e-02,\n",
              "                        9.9254e-02,  1.4457e-01, -2.6222e-01, -8.1384e-02,  1.6831e-01,\n",
              "                       -4.3771e-02, -1.7891e-01, -1.6618e-01, -1.7131e-02, -1.6463e-01,\n",
              "                       -5.5171e-02,  3.6200e-02, -1.3340e-01,  1.4656e-01, -1.8043e-01,\n",
              "                       -2.9115e-01, -3.4692e-01, -1.0872e-01,  1.4860e-01, -1.2208e-01,\n",
              "                       -1.4873e-01,  3.9653e-02],\n",
              "                      [-1.9041e-01,  1.7407e-01, -1.2241e-01, -4.6625e-02,  1.3144e-01,\n",
              "                       -1.3570e-01,  2.1936e-02, -3.8394e-01, -1.1744e-01, -1.3499e-01,\n",
              "                        1.7325e-01, -8.3562e-02, -2.4420e-01, -9.2370e-02, -1.5397e-01,\n",
              "                       -6.8765e-02, -1.0172e-02,  8.6544e-02, -3.6255e-02, -1.1138e-01,\n",
              "                       -1.6428e-01,  1.9904e-03, -1.9100e-01, -6.3275e-02, -2.8931e-01,\n",
              "                       -3.3340e-02, -4.9576e-02, -1.4337e-02, -1.3495e-01, -9.1220e-02,\n",
              "                       -1.3584e-01, -2.1641e-02],\n",
              "                      [-2.4437e-01, -1.2277e-02, -5.0102e-02, -2.2607e-01, -6.5675e-02,\n",
              "                       -3.8744e-02, -4.9716e-01,  6.3753e-01,  5.9560e-01, -1.0135e-01,\n",
              "                        2.0442e-01,  1.7997e-01,  6.5887e-01,  1.0154e-01,  1.0953e-01,\n",
              "                       -1.2408e-01,  5.1620e-01, -4.4331e-01, -5.2405e-01,  7.0560e-02,\n",
              "                        2.2233e+00,  1.2657e-02, -2.1962e-01,  3.9143e-02,  3.4193e-01,\n",
              "                        5.8715e-01, -4.7589e-01, -1.5213e-01, -4.3670e-02,  6.8310e-02,\n",
              "                       -3.6427e-02, -4.9082e-02],\n",
              "                      [ 6.6292e-02,  4.3669e-03,  4.9155e-02, -1.3496e-01, -1.4682e-01,\n",
              "                        3.8640e-02, -2.3369e-01, -2.2154e-01, -2.4723e-01,  1.1342e-01,\n",
              "                       -8.3225e-02,  1.1123e-01,  7.5221e-03, -1.2737e-01,  1.2132e-01,\n",
              "                       -1.3532e-01, -9.6045e-02,  3.4595e-02, -2.2565e-01, -1.2139e-01,\n",
              "                       -1.8820e-02, -1.4783e-01,  2.1940e-02, -1.3930e-01, -2.9062e-01,\n",
              "                       -2.0129e-01, -1.0957e-02, -1.6563e-01, -1.1890e-02,  1.5152e-01,\n",
              "                        2.5589e-02, -1.4832e-01],\n",
              "                      [-1.3948e-01,  7.6682e-02,  1.0784e-02,  4.0666e-01, -2.1281e-01,\n",
              "                       -2.2654e-01, -4.4497e-02,  4.8790e-01,  4.2391e-01, -1.6763e-01,\n",
              "                        6.2178e-02,  6.7839e-01,  6.1868e-01,  1.6203e-01, -2.0679e-02,\n",
              "                       -3.7937e-02,  5.3913e-01, -1.2250e-01,  1.9281e-01,  9.5909e-02,\n",
              "                        2.1428e+00, -1.1284e-01,  2.9923e-02,  1.1309e-01,  4.2456e-01,\n",
              "                        4.4893e-01,  2.4323e-02, -1.2437e-01, -1.0016e-01,  1.2300e-01,\n",
              "                       -2.7872e-02,  2.1511e-02],\n",
              "                      [-2.2820e-01,  7.0524e-02,  1.5107e-02, -2.6710e-01, -5.3710e-02,\n",
              "                       -1.9466e-01, -3.8663e-01, -1.0969e-01, -4.2803e-01,  7.0078e-04,\n",
              "                        1.9176e-02,  1.4381e-01, -2.1112e-01, -7.7590e-02,  2.4644e-01,\n",
              "                       -3.2150e-01, -3.7589e-01, -2.1688e-01, -4.3224e-01,  8.7182e-02,\n",
              "                        1.4799e+00, -7.9430e-02, -2.9131e-01, -3.0907e-02, -6.2679e-02,\n",
              "                       -1.2338e-01, -2.9277e-01, -3.4928e-01,  3.6935e-02, -1.0998e-01,\n",
              "                       -3.1554e-01, -1.5378e-01],\n",
              "                      [ 2.8056e-01,  1.0298e-01,  9.0509e-02,  1.0290e-01, -9.6386e-02,\n",
              "                        2.1108e-01,  3.5505e-01, -1.8398e-01,  9.3180e-02,  1.8632e-01,\n",
              "                        3.9277e-02, -3.8111e+00, -1.8875e-01,  1.8127e-01,  1.0270e-03,\n",
              "                        4.2207e-01,  1.9062e-01,  1.8841e-01,  1.5796e-01, -4.4883e-02,\n",
              "                       -6.7294e-01,  1.0846e-02,  3.9360e-01, -3.3523e-02, -2.2022e-01,\n",
              "                        2.2541e-01,  1.6427e-01,  3.4892e-01,  6.4142e-03, -1.1510e-01,\n",
              "                        1.4240e-01,  1.7171e-01],\n",
              "                      [-2.6726e-01, -5.7167e-02, -3.1656e-02, -1.1946e-01, -1.0838e-01,\n",
              "                        9.3198e-01, -1.3643e-01, -1.8886e-02,  8.4478e-02,  1.2404e-01,\n",
              "                       -2.5048e-02,  5.2095e-02, -1.1329e+00, -1.0295e-01, -3.5161e-02,\n",
              "                        2.3827e-01, -6.9906e-01, -3.9076e-02, -4.7984e-02, -1.4865e-01,\n",
              "                        1.6634e+00,  9.9781e-02,  1.3192e-01,  3.0438e-02,  2.3712e-01,\n",
              "                       -1.0059e+00, -4.9798e-01, -1.1409e-01, -2.0332e-01,  3.8481e-03,\n",
              "                        4.5831e-02,  1.1968e-01],\n",
              "                      [ 4.9470e-02,  1.4215e-01, -1.9234e-01, -9.4245e-02, -1.4741e-01,\n",
              "                       -1.3249e-01, -5.0938e-02, -1.6467e-01,  1.2171e-01, -2.0256e-02,\n",
              "                       -1.3557e-01,  1.5168e-01, -1.3717e-01,  1.5391e-01,  1.5378e-02,\n",
              "                       -3.7315e-02, -9.3907e-02,  1.1238e-01, -1.3650e-01, -5.1198e-02,\n",
              "                        1.3743e-03,  5.7242e-02,  6.1147e-02, -1.2338e-01,  6.0478e-02,\n",
              "                       -2.9853e-02, -2.1374e-01,  7.6507e-02,  1.0696e-01, -6.6386e-02,\n",
              "                        5.9126e-02, -1.9819e-02],\n",
              "                      [-4.0295e-01, -9.9728e-02,  1.5065e-01, -5.2090e-01, -3.3342e-02,\n",
              "                       -2.7099e-01, -2.2378e-01,  3.0221e-03, -6.6091e-02,  8.8057e-02,\n",
              "                       -6.6432e-02, -1.5048e-01, -3.3178e-02, -3.5029e-02, -1.5940e-01,\n",
              "                       -3.8509e-01, -1.1197e-01, -4.6832e-01, -3.5622e-01, -7.9186e-02,\n",
              "                       -6.2204e-02, -1.3529e-01, -3.0749e-01, -1.0595e-01, -4.7007e-02,\n",
              "                       -2.2822e-01, -3.4326e-01, -1.1887e-01,  1.5957e-01,  9.4121e-02,\n",
              "                       -1.4464e-01,  2.5876e-02],\n",
              "                      [ 3.0323e-02,  3.7706e-03, -5.2771e-02, -2.3193e-01, -4.6129e-02,\n",
              "                       -2.2817e-01, -8.5045e-03, -6.9073e-02, -3.7398e-02, -9.0462e-02,\n",
              "                        8.5997e-02,  1.5274e-02, -1.8259e-01, -2.9183e-02,  5.2078e-02,\n",
              "                       -1.4047e-01, -1.0584e-01, -1.9564e-01, -1.3705e-01, -3.4744e-02,\n",
              "                        2.3748e-02, -4.6585e-02, -9.3674e-02,  7.8592e-02,  9.2317e-02,\n",
              "                        5.8943e-03,  3.0613e-02, -1.3927e-01, -7.3919e-02,  8.6877e-02,\n",
              "                       -1.4113e-01, -1.2866e-01]])),\n",
              "             ('fc5.bias',\n",
              "              tensor([ 0.3193, -0.1219,  0.0862, -0.4454,  0.0807, -0.2712, -0.2882,  0.4156,\n",
              "                      -0.1947,  0.8115,  0.0226, -0.1691,  1.1134, -0.0739, -0.4138, -0.0260])),\n",
              "             ('fc6.weight',\n",
              "              tensor([[ 1.5330e-01, -2.4020e-01, -1.2056e-02, -3.5783e-01,  6.9426e-02,\n",
              "                       -2.4594e-01,  1.0454e-01,  1.3751e-01,  1.1009e-01, -2.2691e-01,\n",
              "                       -3.6441e-02, -1.9113e-01, -5.2798e-02,  2.6887e-01, -1.5455e-01,\n",
              "                        2.0069e-03],\n",
              "                      [ 1.0841e-01,  1.4144e-01, -2.7565e-03, -1.3983e-02, -3.1013e-03,\n",
              "                       -1.4385e-01, -7.8365e-03, -1.0410e-01,  6.5991e-02, -4.1580e-01,\n",
              "                       -2.6160e-01,  2.6510e-01, -4.1876e-02, -2.2035e-01,  3.4618e-02,\n",
              "                       -1.6395e-01],\n",
              "                      [ 3.8276e-01, -3.2386e-01,  4.0157e-01,  9.3354e-05,  3.1408e-01,\n",
              "                       -5.3821e-02,  1.3031e-01,  6.5765e-01,  1.0369e-01,  5.6848e-01,\n",
              "                        2.5191e-01, -3.3519e-01,  1.2233e+00,  1.5977e-01, -8.6930e-02,\n",
              "                       -1.5442e-01],\n",
              "                      [-1.6700e-01,  4.7016e-01, -4.4566e-02, -1.5492e-02, -2.2373e-01,\n",
              "                        3.2969e-02, -1.3500e-01, -3.1237e-01, -1.7937e-02,  1.6747e-01,\n",
              "                        1.6795e-01,  5.1813e-01, -9.4927e-01,  2.2502e-01, -1.6950e-01,\n",
              "                        1.0567e-02],\n",
              "                      [ 2.5827e-01, -2.5700e-01,  4.7015e-02, -1.3801e-01, -1.0405e-02,\n",
              "                       -1.6306e-02,  4.6401e-02, -1.1853e-01, -1.1154e-01, -3.1882e-01,\n",
              "                        6.2032e-03, -7.0810e-03,  1.0634e+00, -3.5275e-02, -2.1727e-01,\n",
              "                       -1.6365e-01],\n",
              "                      [ 1.5168e-01, -4.2246e-01, -3.3480e-01,  2.8080e-02, -2.1178e-01,\n",
              "                        1.2813e-02,  1.9233e-01, -1.9414e-01, -5.4735e-02,  2.3776e-01,\n",
              "                       -1.6494e-01, -8.4499e-01,  1.2815e-01, -2.7768e-01,  1.9598e-01,\n",
              "                        7.7600e-02],\n",
              "                      [-8.9166e-02, -9.7236e-02,  6.1820e-02, -2.2737e-01, -1.9297e-01,\n",
              "                       -1.6992e-01,  9.7314e-02, -9.4301e-02, -1.7140e-02,  1.3673e-01,\n",
              "                       -1.8976e-01, -2.5056e-01, -2.0961e-01, -8.7255e-03, -2.4279e-01,\n",
              "                        2.6573e-01],\n",
              "                      [-1.6954e-01, -3.7262e-01, -4.1294e-01,  1.2801e-01, -3.9547e-01,\n",
              "                        3.1521e-02, -6.8603e-02, -4.8719e-01,  2.0144e-01, -1.0309e-01,\n",
              "                       -5.6134e-02, -4.5397e-01, -2.6077e-02,  1.6679e-01,  3.2993e-02,\n",
              "                        7.2331e-02]])),\n",
              "             ('fc6.bias',\n",
              "              tensor([-0.5352, -0.1595,  0.6368, -0.4452, -0.4958,  0.5965, -0.2840, -0.3473])),\n",
              "             ('fc7.weight',\n",
              "              tensor([[ 0.0228,  0.0874,  0.4325, -0.5976,  0.0999,  0.4121,  0.0352, -0.0851]])),\n",
              "             ('fc7.bias', tensor([0.8520]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_int8.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv4HdsOGYC-g",
        "outputId": "da0131ec-0c5c-45d8-aae8-dfff0021286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.scale', tensor(1.)),\n",
              "             ('fc1.zero_point', tensor(0)),\n",
              "             ('fc1._packed_params.dtype', torch.qint8),\n",
              "             ('fc1._packed_params._packed_params',\n",
              "              (tensor([[-0.0357, -0.3685],\n",
              "                       [-0.7252, -0.3685],\n",
              "                       [-0.0238, -0.1545],\n",
              "                       [ 0.0119,  0.9748],\n",
              "                       [-0.4874,  0.0000],\n",
              "                       [ 0.9273, -0.1070],\n",
              "                       [-0.1902, -0.1545],\n",
              "                       [ 0.9986,  0.3210],\n",
              "                       [ 0.7014, -0.3210],\n",
              "                       [ 0.3685, -0.2972],\n",
              "                       [ 0.0476, -0.4993],\n",
              "                       [ 0.7608, -0.2972],\n",
              "                       [-0.2140, -0.2853],\n",
              "                       [ 1.0580, -0.4042],\n",
              "                       [-0.3329,  0.1308],\n",
              "                       [ 0.3566, -0.0951],\n",
              "                       [ 0.0119, -0.5231],\n",
              "                       [ 0.3685,  0.5587],\n",
              "                       [-0.1783,  0.7489],\n",
              "                       [ 0.1189,  0.5468],\n",
              "                       [-0.7846, -0.1308],\n",
              "                       [ 0.4993, -0.5231],\n",
              "                       [-0.3329, -0.3566],\n",
              "                       [ 0.4636,  0.9154],\n",
              "                       [-0.2378,  0.0000],\n",
              "                       [-0.7014, -0.1308],\n",
              "                       [ 0.3091,  0.4042],\n",
              "                       [ 0.9391,  0.4874],\n",
              "                       [ 0.0238, -0.0951],\n",
              "                       [-0.0713,  0.0357],\n",
              "                       [ 0.8559, -0.0119],\n",
              "                       [ 0.5112, -0.3685],\n",
              "                       [-0.6419,  0.1902],\n",
              "                       [-0.7608,  0.0357],\n",
              "                       [ 0.3923, -0.9629],\n",
              "                       [ 0.3091, -0.0713],\n",
              "                       [ 0.3804,  0.6182],\n",
              "                       [ 0.5112,  0.0476],\n",
              "                       [-0.7370, -0.2853],\n",
              "                       [-0.4755, -0.1189],\n",
              "                       [-0.6419,  0.1070],\n",
              "                       [ 0.0238, -0.2021],\n",
              "                       [-0.7965,  0.4993],\n",
              "                       [ 0.5706,  0.5112],\n",
              "                       [-0.0594, -0.0594],\n",
              "                       [ 0.1070, -0.2496],\n",
              "                       [ 0.4874,  0.0238],\n",
              "                       [-0.4042,  0.0238],\n",
              "                       [ 0.2140, -0.0238],\n",
              "                       [ 0.5112, -0.2853],\n",
              "                       [ 0.4280,  0.6063],\n",
              "                       [ 0.6776, -0.0119],\n",
              "                       [-0.7370, -0.3447],\n",
              "                       [-0.5825, -0.5468],\n",
              "                       [-0.5587,  0.0000],\n",
              "                       [ 0.5468, -0.5706],\n",
              "                       [-0.7370, -0.0951],\n",
              "                       [ 0.2615, -0.8322],\n",
              "                       [ 0.8559, -0.0951],\n",
              "                       [ 0.3804,  0.3091],\n",
              "                       [ 0.3210,  0.5112],\n",
              "                       [ 0.8678, -0.0238],\n",
              "                       [-0.7965, -0.1189],\n",
              "                       [ 0.2972,  0.0476],\n",
              "                       [ 0.6776, -0.0357],\n",
              "                       [ 0.7608, -0.0238],\n",
              "                       [ 1.2482,  0.0119],\n",
              "                       [-0.1783,  0.0000],\n",
              "                       [-0.9986,  0.0476],\n",
              "                       [ 0.3566, -0.5350],\n",
              "                       [-0.0594, -0.5350],\n",
              "                       [-0.4517,  0.7965],\n",
              "                       [-0.1902,  0.1783],\n",
              "                       [ 1.0461,  0.0594],\n",
              "                       [ 0.4874, -0.3447],\n",
              "                       [-0.5825,  0.1783],\n",
              "                       [-0.4280, -0.0951],\n",
              "                       [ 0.8916, -0.0713],\n",
              "                       [-0.1070,  0.9748],\n",
              "                       [-0.6657, -0.6776],\n",
              "                       [-0.1783,  0.1070],\n",
              "                       [-0.2972,  0.4874],\n",
              "                       [-0.2734, -0.6895],\n",
              "                       [ 0.0832, -0.3804],\n",
              "                       [-0.8322, -0.8322],\n",
              "                       [ 0.3091, -0.0476],\n",
              "                       [ 0.2378,  0.0594],\n",
              "                       [ 0.0238,  0.0238],\n",
              "                       [-0.9391, -0.0713],\n",
              "                       [ 0.2259, -0.1308],\n",
              "                       [ 0.3091, -0.2378],\n",
              "                       [-0.0594, -0.6182],\n",
              "                       [ 1.0937,  0.0119],\n",
              "                       [ 0.2259, -0.0951],\n",
              "                       [ 1.1888, -0.4517],\n",
              "                       [-0.8797,  0.0119],\n",
              "                       [ 0.4636, -0.7252],\n",
              "                       [ 0.2734, -0.1664],\n",
              "                       [ 0.2378,  0.2140],\n",
              "                       [-0.9748, -0.0832],\n",
              "                       [ 0.0832, -0.8678],\n",
              "                       [ 0.8559, -0.1902],\n",
              "                       [-0.0238,  0.2734],\n",
              "                       [-0.1189, -1.0580],\n",
              "                       [ 0.4755, -0.0238],\n",
              "                       [-0.9154, -0.6895],\n",
              "                       [ 0.2378, -0.0713],\n",
              "                       [-0.5587, -0.1664],\n",
              "                       [ 0.1070, -0.7608],\n",
              "                       [ 0.4993,  0.9748],\n",
              "                       [ 0.7014,  0.4993],\n",
              "                       [ 0.2021,  0.0119],\n",
              "                       [ 1.0224, -0.0594],\n",
              "                       [ 0.2140, -0.1308],\n",
              "                       [-0.4874,  0.4755],\n",
              "                       [ 0.5825, -0.3329],\n",
              "                       [ 0.6301, -0.4399],\n",
              "                       [ 0.1664,  0.0832],\n",
              "                       [ 0.8797,  0.2021],\n",
              "                       [-0.3685, -0.8797],\n",
              "                       [ 0.8322,  0.0951],\n",
              "                       [-0.0951,  0.3447],\n",
              "                       [ 0.5587, -0.6657],\n",
              "                       [ 0.8322, -0.4280],\n",
              "                       [ 1.5098,  0.8916],\n",
              "                       [-0.1664, -0.2259],\n",
              "                       [-0.8322,  0.4399],\n",
              "                       [ 0.0238,  0.8916]], size=(128, 2), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.011887884698808193,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-5.3108e-01,  2.4700e-01,  2.7344e-01, -3.7837e-01,  1.0288e-01,\n",
              "                        2.6775e-01, -1.0328e-01,  4.0630e-01, -2.0215e-01,  7.3900e-01,\n",
              "                        5.4134e-01, -2.2675e-01,  1.9400e-01,  3.7060e-02,  1.3986e-01,\n",
              "                       -3.8499e-02, -5.1175e-01, -6.4161e-01, -3.2293e-01,  1.7506e-02,\n",
              "                       -5.3674e-01, -4.0158e-01,  3.7544e-02, -3.5627e-01,  7.1253e-01,\n",
              "                       -4.6968e-01,  1.5091e-01,  3.2218e-01, -6.0395e-01, -5.6249e-01,\n",
              "                        6.6622e-04, -6.4277e-01,  1.8685e-01, -4.7357e-01,  4.3536e-01,\n",
              "                       -3.6302e-01, -6.8313e-01,  3.3249e-01, -2.1856e-01,  1.4165e-01,\n",
              "                       -5.3141e-01, -2.1486e-01, -5.4902e-01, -6.4705e-01,  8.2692e-01,\n",
              "                       -2.5899e-01,  4.1420e-01, -3.7899e-01,  5.2927e-01, -6.0321e-01,\n",
              "                       -6.7052e-01, -5.1138e-01,  1.3834e-02,  1.4084e-01, -4.1113e-01,\n",
              "                       -7.7193e-01, -6.1843e-01, -6.1855e-01, -4.7071e-01,  1.0223e-01,\n",
              "                       -5.9950e-01, -4.5070e-01, -6.3252e-01, -3.7765e-01, -5.3303e-01,\n",
              "                       -5.8132e-01,  3.5766e-01, -9.3785e-02, -6.9065e-01,  3.8343e-01,\n",
              "                        5.2188e-01, -5.4864e-01, -6.5857e-01,  2.4317e-01, -6.4902e-01,\n",
              "                       -4.8816e-01, -1.7525e-01, -4.8249e-01, -5.3374e-01,  3.1002e-01,\n",
              "                       -5.9564e-01, -9.0548e-02, -3.8007e-01, -3.7959e-01, -3.4180e-01,\n",
              "                        3.5942e-02, -2.6659e-01, -1.9208e-01, -5.5244e-01,  8.1199e-01,\n",
              "                       -1.4888e-01, -4.2803e-01, -5.6661e-01, -9.8942e-02,  4.6425e-01,\n",
              "                       -5.9655e-01, -5.5964e-01,  7.6118e-02,  2.4850e-01, -5.8190e-01,\n",
              "                       -4.9298e-01,  3.1473e-01,  1.4450e-01, -6.7127e-01,  2.1111e-01,\n",
              "                       -5.1001e-01, -2.0845e-01,  2.2201e-01, -5.0876e-01, -3.8717e-01,\n",
              "                       -7.4759e-01, -3.5764e-01,  2.9791e-01,  6.2587e-02, -4.8337e-01,\n",
              "                       -2.3480e-01, -7.0901e-01, -3.2022e-01,  4.7303e-01, -1.4940e-01,\n",
              "                       -5.7936e-01, -1.4785e-01, -8.3813e-01, -3.4240e-01, -1.3542e-01,\n",
              "                       -4.1672e-01, -4.7378e-01, -3.4631e-01], requires_grad=True))),\n",
              "             ('fc2.scale', tensor(1.)),\n",
              "             ('fc2.zero_point', tensor(0)),\n",
              "             ('fc2._packed_params.dtype', torch.qint8),\n",
              "             ('fc2._packed_params._packed_params',\n",
              "              (tensor([[ 0.3857, -0.0203, -0.0609,  ...,  0.0406, -0.2842, -0.3654],\n",
              "                       [-0.1421, -0.1421, -0.1827,  ...,  0.0609,  0.2436,  0.4669],\n",
              "                       [-0.0812, -0.1421, -0.0406,  ..., -0.1015, -0.1827, -0.2030],\n",
              "                       ...,\n",
              "                       [ 0.3857,  0.1827,  0.0203,  ...,  0.2436, -0.3451, -0.1624],\n",
              "                       [ 0.4263,  0.2436,  0.0203,  ..., -0.0203,  0.3248, -0.0609],\n",
              "                       [ 0.0609, -0.1421,  0.0000,  ...,  0.0812, -0.1015, -0.1015]],\n",
              "                      size=(64, 128), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.020298995077610016,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.0217, -0.1359, -0.1367, -0.0767, -0.1035, -0.2515,  0.0309,  0.0488,\n",
              "                       -0.1604, -0.1367, -0.1879, -0.1354, -0.1375, -0.1765, -0.1497, -0.0099,\n",
              "                       -0.1271, -0.0887, -0.1356, -0.0551, -0.0680,  0.1945, -0.1017, -0.2694,\n",
              "                       -0.2331, -0.1612, -0.0728,  0.0752, -0.0223, -0.2910, -0.0222, -0.0833,\n",
              "                       -0.1074, -0.1271, -0.2270, -0.1087, -0.0877, -0.0268, -0.1351, -0.2977,\n",
              "                       -0.0531, -0.0936, -0.1600, -0.1860, -0.1099, -0.1956, -0.1774, -0.0586,\n",
              "                       -0.1013, -0.0087, -0.2094, -0.1970, -0.0504, -0.2358, -0.2790,  0.1268,\n",
              "                       -0.1147,  0.1019, -0.0715, -0.0408, -0.1186,  0.0514, -0.2951, -0.0863],\n",
              "                      requires_grad=True))),\n",
              "             ('fc3.scale', tensor(1.)),\n",
              "             ('fc3.zero_point', tensor(0)),\n",
              "             ('fc3._packed_params.dtype', torch.qint8),\n",
              "             ('fc3._packed_params._packed_params',\n",
              "              (tensor([[ 0.0331, -0.0992,  0.0000,  ..., -0.0331,  0.0992,  0.0331],\n",
              "                       [ 0.0000, -0.4301, -0.0992,  ...,  0.7609, -0.0331,  0.0662],\n",
              "                       [-0.8271, -0.4301,  0.0000,  ..., -1.3564, -0.8601, -0.0331],\n",
              "                       ...,\n",
              "                       [-0.1654,  0.2977,  0.0000,  ...,  0.2316,  0.9594,  0.0331],\n",
              "                       [-0.2977,  0.0331,  0.0331,  ...,  0.1985,  0.8932, -0.0662],\n",
              "                       [ 0.0662, -0.0992,  0.0662,  ..., -0.0992, -0.5293,  0.0000]],\n",
              "                      size=(64, 64), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.03308248147368431,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-2.0294e-01, -3.3308e-01,  2.4274e-01, -1.0133e-01, -1.7262e-01,\n",
              "                       -2.3086e-01, -4.8288e-02,  4.2867e-02,  3.2239e-01,  2.1096e-01,\n",
              "                       -9.3146e-02, -8.4195e-02, -2.2106e-01, -3.5766e-02,  2.8280e-01,\n",
              "                       -1.7843e-01, -2.0468e-01, -7.4813e-02, -1.3435e+00,  1.3522e-01,\n",
              "                       -9.1660e-02,  5.1628e-02, -1.5493e-01, -2.5197e-01, -3.4978e-01,\n",
              "                        2.0210e-01, -3.5688e-01, -1.8639e-01, -2.1030e-01, -2.8606e-01,\n",
              "                        8.2848e-02,  6.5852e-04,  5.6948e-02, -3.6549e-01, -1.6088e-01,\n",
              "                        1.7226e-02, -1.1607e-01, -4.5733e-01, -5.5511e-02, -3.8372e-02,\n",
              "                        2.6160e-01, -1.0440e-01, -5.9630e-02, -2.5984e-01, -2.1666e-01,\n",
              "                        3.6073e-02,  3.4878e-02, -1.6761e-01, -4.0203e-02,  8.9323e-01,\n",
              "                       -1.3037e-01,  2.5348e-02, -7.8261e-01, -3.3209e-01,  1.4006e-01,\n",
              "                        1.1700e-02, -9.1381e-02, -5.1082e-02, -4.1714e-01,  3.2196e-03,\n",
              "                       -8.0616e-02, -7.6607e-02, -1.2390e-01, -2.8499e-01],\n",
              "                      requires_grad=True))),\n",
              "             ('fc4.scale', tensor(1.)),\n",
              "             ('fc4.zero_point', tensor(0)),\n",
              "             ('fc4._packed_params.dtype', torch.qint8),\n",
              "             ('fc4._packed_params._packed_params',\n",
              "              (tensor([[ 0.0366, -0.0733,  1.3559,  ...,  0.2565,  0.2932,  0.0733],\n",
              "                       [ 0.0000, -0.1099, -0.0366,  ..., -0.0733,  0.0733, -0.1466],\n",
              "                       [-0.0733, -0.0733, -0.0733,  ..., -0.0366, -0.1099, -0.0366],\n",
              "                       ...,\n",
              "                       [-0.0366, -0.0366, -0.1099,  ...,  0.0366, -0.1099, -0.1832],\n",
              "                       [-0.0733,  0.1099, -0.0366,  ..., -0.1099, -0.0366,  0.0000],\n",
              "                       [ 0.0733, -0.0733,  0.1099,  ..., -0.2565, -0.2565, -0.0366]],\n",
              "                      size=(32, 64), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.03664489835500717,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.1386, -0.0154, -0.0905, -0.0393, -0.1491, -0.4994, -0.0891, -0.0587,\n",
              "                       -0.0233, -0.1456, -0.1120, -0.0341,  0.4917, -0.2749,  0.0120, -0.2743,\n",
              "                        0.3082, -0.2317,  0.0063,  0.0584, -0.7562, -0.1433, -0.1292, -0.1726,\n",
              "                       -0.1761,  0.4071, -0.1261, -0.1872, -0.0035, -0.0106, -0.2741, -0.2421],\n",
              "                      requires_grad=True))),\n",
              "             ('fc5.scale', tensor(1.)),\n",
              "             ('fc5.zero_point', tensor(0)),\n",
              "             ('fc5._packed_params.dtype', torch.qint8),\n",
              "             ('fc5._packed_params._packed_params',\n",
              "              (tensor([[-0.0598,  0.0598,  0.0299, -0.1196, -0.1196,  0.0299,  0.0598, -0.0299,\n",
              "                        -0.4783,  0.0598,  0.0897, -0.1495,  0.0000, -0.0598,  0.0299, -0.0299,\n",
              "                        -0.2391, -0.0598, -0.1495, -0.1196,  0.0000,  0.0897, -0.3288,  0.0000,\n",
              "                        -0.2989, -0.2092, -0.1196, -0.2391, -0.0897,  0.0598,  0.0000,  0.0000],\n",
              "                       [ 0.2092, -0.0299, -0.0897,  0.2391, -0.0598,  0.1793,  0.0299, -0.1495,\n",
              "                         0.0000,  0.0000,  0.2092, -3.2880, -0.0598, -0.0897,  0.2391,  0.3288,\n",
              "                         0.3288,  0.2391,  0.1793, -0.1196, -0.9864,  0.0598,  0.1196,  0.0000,\n",
              "                        -0.1495,  0.2690,  0.1793,  0.2690,  0.0299,  0.0000, -0.0299,  0.0598],\n",
              "                       [-0.2690,  0.0897,  0.0897, -0.3587,  0.1196, -0.1196, -0.0299, -0.4783,\n",
              "                        -0.1495,  0.1793, -0.1196, -0.0897, -0.2989,  0.1196,  0.0000, -0.1196,\n",
              "                        -0.0897, -0.1495, -0.2690,  0.0897,  0.8071, -0.0299, -0.1793, -0.0598,\n",
              "                        -0.2391, -0.4484, -0.3587, -0.0299,  0.0299, -0.1793,  0.1196, -0.1495],\n",
              "                       [-0.1495, -0.1495, -0.0897, -0.2092,  0.0598, -0.2989, -0.2092, -0.2092,\n",
              "                        -0.2391, -0.1196,  0.1793, -0.0598, -0.3587, -0.1793, -0.2391, -0.3886,\n",
              "                        -0.2391, -0.2092, -0.2391,  0.0299,  0.0897, -0.1196, -0.4185,  0.1196,\n",
              "                        -0.0897, -0.4484, -0.1495, -0.3886, -0.0598,  0.1495,  0.6277, -0.0299],\n",
              "                       [-0.1495,  0.0000, -0.0897, -0.2092, -0.2092, -0.2092, -0.0598, -0.0897,\n",
              "                        -0.1196, -0.0897, -0.0299,  0.0000, -0.1196,  0.1196,  0.0299, -0.0598,\n",
              "                        -0.0897,  0.0000, -0.1495,  0.0897,  1.4348,  0.1196,  0.0000,  0.1793,\n",
              "                        -0.1495,  0.0000, -0.0598, -0.0897,  0.0000, -0.2391, -0.1793, -0.2391],\n",
              "                       [-0.0598,  0.1495, -0.0299, -0.0598, -0.0897, -0.3587,  0.0000, -0.0299,\n",
              "                        -0.3288,  0.0598,  0.0897,  0.1495, -0.2690, -0.0897,  0.1793, -0.0299,\n",
              "                        -0.1793, -0.1793, -0.0299, -0.1793, -0.0598,  0.0299, -0.1196,  0.1495,\n",
              "                        -0.1793, -0.2989, -0.3587, -0.1196,  0.1495, -0.1196, -0.1495,  0.0299],\n",
              "                       [-0.1793,  0.1793, -0.1196, -0.0598,  0.1196, -0.1495,  0.0299, -0.3886,\n",
              "                        -0.1196, -0.1495,  0.1793, -0.0897, -0.2391, -0.0897, -0.1495, -0.0598,\n",
              "                         0.0000,  0.0897, -0.0299, -0.1196, -0.1495,  0.0000, -0.1793, -0.0598,\n",
              "                        -0.2989, -0.0299, -0.0598,  0.0000, -0.1495, -0.0897, -0.1495, -0.0299],\n",
              "                       [-0.2391,  0.0000, -0.0598, -0.2391, -0.0598, -0.0299, -0.5081,  0.6277,\n",
              "                         0.5978, -0.0897,  0.2092,  0.1793,  0.6576,  0.0897,  0.1196, -0.1196,\n",
              "                         0.5081, -0.4484, -0.5380,  0.0598,  2.2119,  0.0000, -0.2092,  0.0299,\n",
              "                         0.3288,  0.5978, -0.4783, -0.1495, -0.0299,  0.0598, -0.0299, -0.0598],\n",
              "                       [ 0.0598,  0.0000,  0.0598, -0.1495, -0.1495,  0.0299, -0.2391, -0.2092,\n",
              "                        -0.2391,  0.1196, -0.0897,  0.1196,  0.0000, -0.1196,  0.1196, -0.1495,\n",
              "                        -0.0897,  0.0299, -0.2391, -0.1196, -0.0299, -0.1495,  0.0299, -0.1495,\n",
              "                        -0.2989, -0.2092,  0.0000, -0.1793,  0.0000,  0.1495,  0.0299, -0.1495],\n",
              "                       [-0.1495,  0.0897,  0.0000,  0.4185, -0.2092, -0.2391, -0.0299,  0.4783,\n",
              "                         0.4185, -0.1793,  0.0598,  0.6875,  0.6277,  0.1495, -0.0299, -0.0299,\n",
              "                         0.5380, -0.1196,  0.1793,  0.0897,  2.1521, -0.1196,  0.0299,  0.1196,\n",
              "                         0.4185,  0.4484,  0.0299, -0.1196, -0.0897,  0.1196, -0.0299,  0.0299],\n",
              "                       [-0.2391,  0.0598,  0.0299, -0.2690, -0.0598, -0.2092, -0.3886, -0.1196,\n",
              "                        -0.4185,  0.0000,  0.0299,  0.1495, -0.2092, -0.0897,  0.2391, -0.3288,\n",
              "                        -0.3886, -0.2092, -0.4185,  0.0897,  1.4945, -0.0897, -0.2989, -0.0299,\n",
              "                        -0.0598, -0.1196, -0.2989, -0.3587,  0.0299, -0.1196, -0.3288, -0.1495],\n",
              "                       [ 0.2690,  0.0897,  0.0897,  0.0897, -0.0897,  0.2092,  0.3587, -0.1793,\n",
              "                         0.0897,  0.1793,  0.0299, -3.8260, -0.1793,  0.1793,  0.0000,  0.4185,\n",
              "                         0.1793,  0.1793,  0.1495, -0.0598, -0.6875,  0.0000,  0.3886, -0.0299,\n",
              "                        -0.2092,  0.2391,  0.1495,  0.3587,  0.0000, -0.1196,  0.1495,  0.1793],\n",
              "                       [-0.2690, -0.0598, -0.0299, -0.1196, -0.1196,  0.9266, -0.1495, -0.0299,\n",
              "                         0.0897,  0.1196, -0.0299,  0.0598, -1.1359, -0.0897, -0.0299,  0.2391,\n",
              "                        -0.6875, -0.0299, -0.0598, -0.1495,  1.6739,  0.0897,  0.1196,  0.0299,\n",
              "                         0.2391, -1.0163, -0.5081, -0.1196, -0.2092,  0.0000,  0.0598,  0.1196],\n",
              "                       [ 0.0598,  0.1495, -0.1793, -0.0897, -0.1495, -0.1196, -0.0598, -0.1793,\n",
              "                         0.1196, -0.0299, -0.1495,  0.1495, -0.1495,  0.1495,  0.0299, -0.0299,\n",
              "                        -0.0897,  0.1196, -0.1495, -0.0598,  0.0000,  0.0598,  0.0598, -0.1196,\n",
              "                         0.0598, -0.0299, -0.2092,  0.0897,  0.1196, -0.0598,  0.0598, -0.0299],\n",
              "                       [-0.3886, -0.0897,  0.1495, -0.5081, -0.0299, -0.2690, -0.2092,  0.0000,\n",
              "                        -0.0598,  0.0897, -0.0598, -0.1495, -0.0299, -0.0299, -0.1495, -0.3886,\n",
              "                        -0.1196, -0.4783, -0.3587, -0.0897, -0.0598, -0.1495, -0.2989, -0.1196,\n",
              "                        -0.0598, -0.2391, -0.3288, -0.1196,  0.1495,  0.0897, -0.1495,  0.0299],\n",
              "                       [ 0.0299,  0.0000, -0.0598, -0.2391, -0.0598, -0.2391,  0.0000, -0.0598,\n",
              "                        -0.0299, -0.0897,  0.0897,  0.0299, -0.1793, -0.0299,  0.0598, -0.1495,\n",
              "                        -0.1196, -0.2092, -0.1495, -0.0299,  0.0299, -0.0598, -0.0897,  0.0897,\n",
              "                         0.0897,  0.0000,  0.0299, -0.1495, -0.0598,  0.0897, -0.1495, -0.1196]],\n",
              "                      size=(16, 32), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.02989092469215393,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([ 0.3193, -0.1219,  0.0862, -0.4454,  0.0807, -0.2712, -0.2882,  0.4156,\n",
              "                       -0.1947,  0.8115,  0.0226, -0.1691,  1.1134, -0.0739, -0.4138, -0.0260],\n",
              "                      requires_grad=True))),\n",
              "             ('fc6.scale', tensor(1.)),\n",
              "             ('fc6.zero_point', tensor(0)),\n",
              "             ('fc6._packed_params.dtype', torch.qint8),\n",
              "             ('fc6._packed_params._packed_params',\n",
              "              (tensor([[ 0.1535, -0.2399, -0.0096, -0.3550,  0.0672, -0.2495,  0.1055,  0.1343,\n",
              "                         0.1055, -0.2303, -0.0384, -0.1919, -0.0576,  0.2686, -0.1535,  0.0000],\n",
              "                       [ 0.1055,  0.1439,  0.0000, -0.0096,  0.0000, -0.1439, -0.0096, -0.1055,\n",
              "                         0.0672, -0.4126, -0.2590,  0.2686, -0.0384, -0.2207,  0.0384, -0.1631],\n",
              "                       [ 0.3838, -0.3262,  0.4030,  0.0000,  0.3166, -0.0576,  0.1343,  0.6620,\n",
              "                         0.1055,  0.5661,  0.2495, -0.3358,  1.2185,  0.1631, -0.0863, -0.1535],\n",
              "                       [-0.1631,  0.4701, -0.0480, -0.0192, -0.2207,  0.0288, -0.1343, -0.3166,\n",
              "                        -0.0192,  0.1631,  0.1727,  0.5181, -0.9498,  0.2207, -0.1727,  0.0096],\n",
              "                       [ 0.2590, -0.2590,  0.0480, -0.1343, -0.0096, -0.0192,  0.0480, -0.1151,\n",
              "                        -0.1151, -0.3166,  0.0096, -0.0096,  1.0650, -0.0384, -0.2207, -0.1631],\n",
              "                       [ 0.1535, -0.4222, -0.3358,  0.0288, -0.2111,  0.0096,  0.1919, -0.1919,\n",
              "                        -0.0576,  0.2399, -0.1631, -0.8443,  0.1247, -0.2782,  0.1919,  0.0768],\n",
              "                       [-0.0863, -0.0959,  0.0576, -0.2303, -0.1919, -0.1727,  0.0959, -0.0959,\n",
              "                        -0.0192,  0.1343, -0.1919, -0.2495, -0.2111, -0.0096, -0.2399,  0.2686],\n",
              "                       [-0.1727, -0.3742, -0.4126,  0.1247, -0.3934,  0.0288, -0.0672, -0.4893,\n",
              "                         0.2015, -0.1055, -0.0576, -0.4509, -0.0288,  0.1631,  0.0288,  0.0768]],\n",
              "                      size=(8, 16), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.009594402275979519,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.5352, -0.1595,  0.6368, -0.4452, -0.4958,  0.5965, -0.2840, -0.3473],\n",
              "                      requires_grad=True))),\n",
              "             ('fc7.scale', tensor(1.)),\n",
              "             ('fc7.zero_point', tensor(0)),\n",
              "             ('fc7._packed_params.dtype', torch.qint8),\n",
              "             ('fc7._packed_params._packed_params',\n",
              "              (tensor([[ 0.0234,  0.0891,  0.4312, -0.5999,  0.0984,  0.4125,  0.0375, -0.0844]],\n",
              "                      size=(1, 8), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.004687081091105938,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([0.8520], requires_grad=True)))])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "torch.save(model_fp32.state_dict(), \"model_fp32.pt\")\n",
        "torch.save(model_int8.state_dict(), \"model_dynamic_int8.pt\")"
      ],
      "metadata": {
        "id": "5xYBu_Ksqje_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FP32 model size (MB):\", os.path.getsize(\"model_fp32.pt\") / 1e6)\n",
        "print(\"INT8 model size (MB):\", os.path.getsize(\"model_dynamic_int8.pt\") / 1e6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJDtE6zFqKLb",
        "outputId": "982e656e-ca3c-49d3-c944-0214a5cda8d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 model size (MB): 0.066914\n",
            "INT8 model size (MB): 0.025954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What quantize_dynamic() Actually Does:\n",
        "\n",
        "1. Weights are quantized\n",
        "\n",
        "All nn.Linear layer weights are converted from float32 → int8\n",
        "\n",
        "Stored in compressed format\n",
        "\n",
        "2. Activations are quantized at runtime (\"on-the-fly\")\n",
        "During inference only\n",
        "\n",
        "Input activations are not stored as int8, they are converted to int8 just before each layer\n",
        "\n",
        "This is what makes it dynamic quantization"
      ],
      "metadata": {
        "id": "jrrV9qGrqP7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization Logic (Manual INT8)\n",
        "def quantize_tensor(t, num_bits=8):\n",
        "    qmin = -2**(num_bits-1)\n",
        "    qmax = 2**(num_bits-1) - 1\n",
        "    min_val, max_val = t.min(), t.max()\n",
        "    scale = (max_val - min_val) / float(qmax - qmin + 1e-8)\n",
        "    zp = torch.round(-min_val / scale).to(torch.int32)\n",
        "    q_t = torch.clamp(torch.round(t / scale) + zp, qmin, qmax).to(torch.int8)\n",
        "    return q_t, scale, zp"
      ],
      "metadata": {
        "id": "Q0fPQd7xqWAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dequantize_tensor(q_t, scale, zp):\n",
        "    return (q_t.float() - zp) * scale"
      ],
      "metadata": {
        "id": "HCSckbk7qXCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Quantization to Weights Only\n",
        "new_model = BigMLP()"
      ],
      "metadata": {
        "id": "HAbfPFTIqYaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in new_model.named_parameters():\n",
        "    print(name)\n",
        "    print(param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPKeZurAuSUN",
        "outputId": "c7ab3891-fa66-4100-ef78-e608d145fef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1.weight\n",
            "torch.Size([128, 2])\n",
            "fc1.bias\n",
            "torch.Size([128])\n",
            "fc2.weight\n",
            "torch.Size([64, 128])\n",
            "fc2.bias\n",
            "torch.Size([64])\n",
            "fc3.weight\n",
            "torch.Size([64, 64])\n",
            "fc3.bias\n",
            "torch.Size([64])\n",
            "fc4.weight\n",
            "torch.Size([32, 64])\n",
            "fc4.bias\n",
            "torch.Size([32])\n",
            "fc5.weight\n",
            "torch.Size([16, 32])\n",
            "fc5.bias\n",
            "torch.Size([16])\n",
            "fc6.weight\n",
            "torch.Size([8, 16])\n",
            "fc6.bias\n",
            "torch.Size([8])\n",
            "fc7.weight\n",
            "torch.Size([1, 8])\n",
            "fc7.bias\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for (name_fp, param_fp), (name_q, param_q) in zip(model_fp32.named_parameters(), new_model.named_parameters()):\n",
        "\n",
        "        q_param, scale, zp = quantize_tensor(param_fp.data)\n",
        "\n",
        "        dq_param = dequantize_tensor(q_param, scale, zp)\n",
        "\n",
        "        param_q.data.copy_(dq_param)"
      ],
      "metadata": {
        "id": "DyJi93kbqaJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n INT8 Accuracy:\", accuracy(new_model, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsFE4jJTqOcU",
        "outputId": "dd5e5980-bc09-4046-f646-4503d8fac8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " INT8 Accuracy: 0.4300000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compressing FP32 tensors into int8 format to save memory & improve speed\n",
        "\n",
        "Convert a floating point tensor t (e.g., model weights) into INT8 range:\n",
        "from 32-bit floats → 8-bit integers\n",
        "\n",
        "also return the scale and zero point, so you can later reverse it (dequantize)\n",
        "\n",
        "| Step                                | Purpose                            |\n",
        "| ----------------------------------- | ---------------------------------- |\n",
        "| `min/max`                           | Find range of your data            |\n",
        "| `scale`                             | How much 1 INT8 step = in float    |\n",
        "| `zp`                                | Align 0.0 to the right INT8 number |\n",
        "| `quantized = round(t / scale) + zp` | Main formula                       |\n",
        "| `clamp`                             | Keep values between -128 and 127   |\n"
      ],
      "metadata": {
        "id": "E7tzBAuBO3Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model_int8 is NOT truly INT8 model\n",
        "\n",
        "It's still FP32 model —\n",
        "\n",
        "But the weights were passed through a quant–dequant cycle to simulate the loss.\n",
        "\n",
        "This is why we say:\n",
        "\n",
        "This is not real quantized inference — it's quantization simulation."
      ],
      "metadata": {
        "id": "-6f7o6d8WIz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose of this block:\n",
        "\n",
        "Take the trained model (model_fp32),\n",
        "\n",
        "Quantize its weights using quantize_tensor()\n",
        "\n",
        "Then dequantize them back to FP32\n",
        "\n",
        "And load them into a new model (model_int8) for testing\n",
        "\n",
        "It's like saying:\n",
        "\n",
        "\"Compress the weights → decompress them → test how much damage happened\"\n",
        "\n",
        "Basically: simulating quantization impact on your model — without actual INT8 inference.\n",
        "\n",
        "Inference is happening at FP32, since you dequantized the weights.\n",
        "\n",
        "So you are not testing real quantized inference.\n",
        "\n",
        "This is not real quantized inference\n",
        "\n",
        "→ It is a simulation to understand quantization error\n",
        "\n",
        "→ i.e., How much precision loss happens when compressing weights\n",
        "\n",
        "Let's take a simple real-life analogy:\n",
        "\n",
        "You have:\n",
        "\n",
        "- A weight: w = 1.23456789 (high precision)\n",
        "\n",
        "- You quantize (compress): w_q = 123 (INT8)\n",
        "\n",
        "- Then dequantize (decompress): w_deq = 1.23 (approximate value)\n",
        "\n",
        "Now compare:\n",
        "\n",
        "- Original: 1.23456789\n",
        "\n",
        "- After round trip (quant-dequant): 1.23\n",
        "\n",
        "You see the \"damage\" = the difference"
      ],
      "metadata": {
        "id": "imoKTpkdWoEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PTQ Static"
      ],
      "metadata": {
        "id": "VUfOnDxQPj1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Original FP32 Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return torch.sigmoid(self.fc3(x))\n",
        "\n",
        "model_fp32 = MLP()\n",
        "\n",
        "optimizer = torch.optim.Adam(model_fp32.parameters(), lr=0.01)\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "for epoch in range(2000):\n",
        "    model_fp32.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model_fp32(X_train)\n",
        "    loss = loss_fn(out, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # Accuracy\n",
        "def accuracy(model, X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X)\n",
        "        preds = (preds > 0.5).float()\n",
        "        return (preds == y).float().mean().item()\n",
        "\n",
        "print(\"\\nFP32 Accuracy:\", accuracy(model_fp32, X_test, y_test))"
      ],
      "metadata": {
        "id": "ooncvzhbRPTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92221e53-5e89-49ce-9da7-cd05a174db50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FP32 Accuracy: 0.9599999785423279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_tensor(t, num_bits=8):\n",
        "    qmin = -2**(num_bits - 1) #-128\n",
        "    qmax = 2**(num_bits - 1) - 1 #127\n",
        "    min_val, max_val = t.min(), t.max()\n",
        "    scale = (max_val - min_val) / (qmax - qmin + 1e-8)\n",
        "    zp = torch.round(-min_val / scale).to(torch.int32)\n",
        "    q_t = torch.clamp(torch.round(t / scale) + zp, qmin, qmax).to(torch.int8)\n",
        "    return q_t, scale, zp\n",
        "\n",
        "def dequantize_tensor(q_t, scale, zp):\n",
        "    return (q_t.float() - zp) * scale"
      ],
      "metadata": {
        "id": "ETlEwUezvzn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Term                   |  Meaning                                           |\n",
        "| ------------------------- | ---------------------------------------------------- |\n",
        "| `hook`                    | A function that \"hooks into\" model layers            |\n",
        "| `forward_hook`            | Runs when the layer does a forward pass              |\n",
        "| `register_forward_hook()` | Registers the hook on the layer                      |\n",
        "| Used For                  | Getting layer activations, gradients, debugging, etc |\n"
      ],
      "metadata": {
        "id": "5SJpDaJyyGka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose → Collect the min/max activation range of each nn.Linear layer (for quantization calibration).\n",
        "\n",
        "Hook function → During forward pass, captures each layer’s output and stores its min() and max().\n",
        "\n",
        "Registration → Hooks are attached only to nn.Linear layers using register_forward_hook.\n",
        "\n",
        "Forward pass → Calibration samples (X_sample) are passed through the model → hooks record the values.\n",
        "\n",
        "Cleanup → Hooks are removed afterward to avoid memory leaks.\n",
        "\n",
        "Return → Returns a dictionary {layer_name: (min, max)} with activation ranges.\n",
        "\n",
        "Intuition → These min/max values are later used to compute scale and zero-point for quantization."
      ],
      "metadata": {
        "id": "M5TxIWKW0q3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration: Get activation ranges\n",
        "def get_activation_min_max(model, X_sample):\n",
        "    act_ranges = {}\n",
        "    hooks = []\n",
        "\n",
        "    def register_hook(name):\n",
        "        def hook_fn(module, input, output):\n",
        "            act_min = output.min().item()\n",
        "            act_max = output.max().item()\n",
        "            act_ranges[name] = (act_min, act_max)\n",
        "        return hook_fn\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            hooks.append(module.register_forward_hook(register_hook(name)))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        model(X_sample)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    return act_ranges"
      ],
      "metadata": {
        "id": "X9kwWFZzRZeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_calib = X_train[:100]"
      ],
      "metadata": {
        "id": "wtIEL4O3RbuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_ranges = get_activation_min_max(model_fp32, X_calib)"
      ],
      "metadata": {
        "id": "TRYiN4jQRdlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "act_ranges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSfnu4QP3A_9",
        "outputId": "ec65765b-0958-49df-f910-4a12ca3efe79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fc1': (-8.62429428100586, 6.317246437072754),\n",
              " 'fc2': (-18.306520462036133, 18.562524795532227),\n",
              " 'fc3': (-100.20124816894531, 83.4000473022461)}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What’s happening?\n",
        "You loop through each weight in the FP32 model.\n",
        "\n",
        "Quantize it to INT8 with quantize_tensor()\n",
        "\n",
        "Immediately dequantize back to FP32 with dequantize_tensor()\n",
        "(Because we’re not doing true INT8 inference — just simulating quantization loss.)\n",
        "\n",
        "Then copy this dequantized value into the new model’s weights.\n",
        "\n",
        "This tells us: \"What would happen to accuracy if we had compressed the weights?\""
      ],
      "metadata": {
        "id": "zrjfUy0xKnrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedMLP(nn.Module):\n",
        "    def __init__(self, fp_model, act_ranges):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "\n",
        "        # Quantize weights\n",
        "        with torch.no_grad():\n",
        "            for (name_fp, param_fp), (name_q, param_q) in zip(fp_model.named_parameters(), self.named_parameters()):\n",
        "                q_param, scale, zp = quantize_tensor(param_fp.data)\n",
        "                dq_param = dequantize_tensor(q_param, scale, zp)\n",
        "                param_q.data.copy_(dq_param)\n",
        "\n",
        "        # Store activation scales\n",
        "        self.act_scales = {}\n",
        "        for name, (amin, amax) in act_ranges.items():\n",
        "            scale = (amax - amin) / 255.0\n",
        "            zp = round(-amin / scale) if scale > 0 else 0\n",
        "            self.act_scales[name] = (scale, zp)\n",
        "\n",
        "    def quant_act(self, x, layer_name):\n",
        "        scale, zp = self.act_scales[layer_name]\n",
        "        q_x = torch.clamp(torch.round(x / scale) + zp, 0, 255).to(torch.uint8)\n",
        "        dq_x = (q_x.float() - zp) * scale\n",
        "        return dq_x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.quant_act(self.fc1(x), 'fc1'))\n",
        "        x = F.relu(self.quant_act(self.fc2(x), 'fc2'))\n",
        "        return torch.sigmoid(self.fc3(x))"
      ],
      "metadata": {
        "id": "zbTvYzUQRfLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_static_ptq = QuantizedMLP(model_fp32, act_ranges)\n",
        "print(\"STATIC PTQ Accuracy:\", accuracy(model_static_ptq, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB7vXJDfRkD7",
        "outputId": "53a238be-ab91-4665-cc94-fe22160c1e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STATIC PTQ Accuracy: 0.4300000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are we NOT doing real INT8 inference in this code?\n",
        "Because we're doing manual quantization using PyTorch (pure Python code), and:\n",
        "\n",
        "PyTorch’s default layers (like nn.Linear) do NOT support actual INT8 operations on CPU/GPU unless we use special quantized modules.\n",
        "\n",
        "You're doing:\n",
        "\n",
        "✅ Quantization → simulate converting to INT8\n",
        "\n",
        "❌ BUT then Dequantization → you bring it back to FP32\n",
        "\n",
        "✅ FP32 Layer runs → because your model (nn.Linear) expects float tensors\n",
        "\n",
        "When does real INT8 inference happen?\n",
        "\n",
        "Only when you use:\n",
        "\n",
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "torch.quantization.prepare() + convert()\n",
        "\n",
        "These internally replace layers like nn.Linear with quantized versions like:\n",
        "\n",
        "torch.nn.quantized.Linear\n"
      ],
      "metadata": {
        "id": "P9pYGr7lMDBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.quantization\n",
        "# from sklearn.datasets import make_moons\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Step 1: Prepare Dataset\n",
        "# X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "# X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# X = torch.tensor(X, dtype=torch.float32)\n",
        "# y = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Step 2: Define Quantizable MLP\n",
        "# class QuantMLP(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         # MUST use QuantStub and DeQuantStub for static quantization\n",
        "#         self.quant = torch.quantization.QuantStub()\n",
        "#         self.fc1 = nn.Linear(2, 16)\n",
        "#         self.fc2 = nn.Linear(16, 8)\n",
        "#         self.fc3 = nn.Linear(8, 1)\n",
        "#         self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.quant(x)                      # Quantize input\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = torch.sigmoid(self.fc3(x))\n",
        "#         x = self.dequant(x)                    # Dequantize output\n",
        "#         return x\n",
        "\n",
        "# # Step 3: Train FP32 Model\n",
        "# model_fp32 = QuantMLP()\n",
        "# optimizer = torch.optim.Adam(model_fp32.parameters(), lr=0.01)\n",
        "# loss_fn = nn.BCELoss()\n",
        "\n",
        "# for epoch in range(2000):\n",
        "#     model_fp32.train()\n",
        "#     optimizer.zero_grad()\n",
        "#     out = model_fp32(X_train)\n",
        "#     loss = loss_fn(out, y_train)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "# # Step 4: Define Accuracy Function\n",
        "# def accuracy(model, X, y):\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         preds = model(X)\n",
        "#         preds = (preds > 0.5).float()\n",
        "#         return (preds == y).float().mean().item()\n",
        "\n",
        "# print(\"FP32 Accuracy:\", accuracy(model_fp32, X_test, y_test))\n",
        "\n",
        "# # Step 5: Static PTQ (REAL)\n",
        "# model_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # Backend config (CPU)\n",
        "# torch.quantization.prepare(model_fp32, inplace=True)  # Insert observers\n",
        "\n",
        "# # Calibration Step (Run some real data through model)\n",
        "# with torch.no_grad():\n",
        "#     model_fp32(X_train)\n",
        "\n",
        "# torch.quantization.convert(model_fp32, inplace=True)  # Convert to INT8\n",
        "\n",
        "# print(\"INT8 Accuracy:\", accuracy(model_fp32, X_test, y_test))\n",
        "\n",
        "# # Optional: Print model to verify quantized layers\n",
        "# print(model_fp32)\n"
      ],
      "metadata": {
        "id": "UDk2NMQgL4RU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QAT (Quantization Aware Training)"
      ],
      "metadata": {
        "id": "8-9S0TarVKsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch, nn, F: PyTorch essentials for model, layers, and functions.\n",
        "\n",
        "make_moons: Generates 2D binary classification data (two semicircles) – easy to visualize and train.\n",
        "\n",
        "train_test_split: Splits dataset into train and test.\n",
        "\n",
        "StandardScaler: Normalizes inputs to mean=0 and std=1.\n",
        "\n",
        "torch.quantization: Module that gives you all the tools for quantization (like QAT, PTQ, etc).\n",
        "\n"
      ],
      "metadata": {
        "id": "IvCwk3DYQgL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, PyTorch runs models on the CPU using FP32 (32-bit floating point) precision.\n",
        "\n",
        "For real INT8 inference, special hardware backends are required — such as x86 (with AVX2/AVX512), FBGEMM, or QNNPACK.\n",
        "\n",
        "But when you're manually applying quantization and dequantization (i.e., using fake quantization), you're essentially just simulating it.\n",
        "\n",
        "You're asking the question:\n",
        "\n",
        "“If this model were quantized, how different would its output be?”\n",
        "\n",
        "So simulation here means:\n",
        "\n",
        "\"We're not actually running on real quantized hardware, but we can still observe how quantization would affect the model's outputs in software.\""
      ],
      "metadata": {
        "id": "0rBRfdSvXyPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original FP32 Tensor:\n",
        "\n",
        "tensor([ 0.1200, -0.8500,  0.5600, -0.3300,  0.9100])\n",
        "\n",
        "After Quantization to INT8:\n",
        "\n",
        "tensor([127,   0, 127,  75, 127], dtype=torch.int8)\n",
        "\n",
        "After Dequantization:\n",
        "\n",
        "tensor([ 0.0276, -0.8489,  0.0276, -0.3313,  0.0276])\n",
        "\n",
        "absolute Error (FP32 − Dequantized):\n",
        "\n",
        "tensor([0.0924, 0.0011, 0.5324, 0.0013, 0.8824])"
      ],
      "metadata": {
        "id": "R-L-XOn4ZPGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import torch.quantization\n",
        "\n",
        "# Dataset\n",
        "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Accuracy Function\n",
        "def accuracy(model, X, y):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X)\n",
        "        preds = (preds > 0.5).float()\n",
        "        return (preds == y).float().mean().item()\n",
        "\n",
        "# QAT-compatible Model\n",
        "class QATMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.fc1 = nn.Linear(2, 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        x = self.dequant(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "W1QYEkERVHI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = QATMLP()"
      ],
      "metadata": {
        "id": "b_chlvXx1uA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.quantization.get_default_qat_qconfig('fbgemm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szeFpNsB36hK",
        "outputId": "7926cb8b-ad92-488d-8e7b-19c2655dbd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QConfig(activation=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'>, observer=<class 'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define quantization config\n",
        "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
      ],
      "metadata": {
        "id": "PIWtHjxPVYGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare QAT model (insert fake quant nodes)\n",
        "torch.quantization.prepare_qat(model, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JhX7piZVaNB",
        "outputId": "6a2dc210-1c9b-46aa-abaf-e428effdcb3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QATMLP(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (fc1): Linear(\n",
              "    in_features=2, out_features=16, bias=True\n",
              "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    )\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (fc2): Linear(\n",
              "    in_features=16, out_features=8, bias=True\n",
              "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    )\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (fc3): Linear(\n",
              "    in_features=8, out_features=1, bias=True\n",
              "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
              "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
              "    )\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer + Loss\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "id": "iB723i2OVbyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with Fake Quantization\n",
        "for epoch in range(2000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(X_train)\n",
        "    loss = loss_fn(out, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch} | Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SRhcfEoVeQB",
        "outputId": "c8bd4840-7b1d-4d73-e536-10318e09343b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Loss: 0.6883\n",
            "Epoch 500 | Loss: 0.1722\n",
            "Epoch 1000 | Loss: 0.0628\n",
            "Epoch 1500 | Loss: 0.0402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy before convert():\", accuracy(model, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alD7jHqWVgap",
        "outputId": "d051bef3-bcaf-4e3d-d1fc-596195e6fb30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy before convert(): 0.9700000286102295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to real INT8 model\n",
        "model_int8 = torch.quantization.convert(model.eval(), inplace=False)"
      ],
      "metadata": {
        "id": "ifo1qB06VlFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"QAT INT8 Accuracy:\", accuracy(model_int8, X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuFc0dREVnAp",
        "outputId": "52bae617-df3f-4cca-8491-bebd0454e13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QAT INT8 Accuracy: 0.9800000190734863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " In real INT8 inference, your model should directly use:\n",
        "\n",
        "tensor([127, 0, 127, 75, 127], dtype=torch.int8)\n",
        "\n",
        "Here's the truth:\n",
        "\n",
        "These int8 values are meant to be directly consumed by quantized operators (like qlinear, qconv, etc.)\n",
        "\n",
        "hese operators know how to compute using:\n",
        "\n",
        "INT8 weights\n",
        "\n",
        "INT8 activations\n",
        "\n",
        "plus saved scale and zero_point\n",
        "\n",
        "But why do we simulate with dequantization then?\n",
        "Because:\n",
        "\n",
        "In pure PyTorch (without enabling real quantized backends like FBGEMM, QNNPACK), we can't execute real int8 ops.\n",
        "\n",
        "So we do:\n",
        "\n",
        "Quantize → Dequantize → FP32 inference\n",
        "Just to test the accuracy drop before switching to real INT8 path.\n",
        "\n",
        "REAL INT8 inference happens in:\n",
        "\n",
        "PyTorch with torch.quantization.convert() + qengine\n",
        "\n",
        "ONNX Runtime\n",
        "\n",
        "TensorRT\n",
        "\n",
        "TFLite\n",
        "\n",
        "Or mobile deployment"
      ],
      "metadata": {
        "id": "uavbt8BDZ8wE"
      }
    }
  ]
}