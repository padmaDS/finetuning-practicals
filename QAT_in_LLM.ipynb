{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_O5rjY2tF9I"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization as tq\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Step 1: Load a smaller LLM (for demo we use DistilGPT2)\n",
        "model_name = \"distilgpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Define QAT config\n",
        "# qat_config = tq.QConfig(\n",
        "#     activation=tq.FakeQuantize.with_args(observer=tq.MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine),\n",
        "#     weight=tq.FakeQuantize.with_args(observer=tq.MinMaxObserver, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)\n",
        "# )\n",
        "\n",
        "qat_config = tq.QConfig(\n",
        "    activation=tq.FakeQuantize.with_args(\n",
        "        observer=tq.MovingAverageMinMaxObserver,\n",
        "        quant_min=0, quant_max=255,\n",
        "        dtype=torch.quint8, qscheme=torch.per_tensor_affine\n",
        "    ),\n",
        "    weight=tq.FakeQuantize.with_args(\n",
        "        observer=tq.MinMaxObserver,\n",
        "        quant_min=-128, quant_max=127,\n",
        "        dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Explicitly ignore embedding layers\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, nn.Embedding):\n",
        "        module.qconfig = None  # no quantization for embeddings\n",
        "\n",
        "\n",
        "# Step 3: Attach quant/dequant stubs\n",
        "model.qconfig = qat_config\n",
        "\n",
        "model.train()\n",
        "tq.prepare_qat(model, inplace=True)   # Model now has fake quant ops\n",
        "\n",
        "# Step 4: Fine-tune the quantization-aware model\n",
        "inputs = tokenizer(\"Quantization Aware Training on LLMs!\", return_tensors=\"pt\")\n",
        "\n",
        "labels = inputs[\"input_ids\"]\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# model.train()\n",
        "\n",
        "for step in range(50):   # tiny fine-tuning loop\n",
        "    outputs = model(**inputs, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step} | Loss: {loss.item()}\")\n",
        "\n",
        "# Step 5: Convert to fully quantized\n",
        "qat_model = tq.convert(model.eval(), inplace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN78zHsxN-xO"
      },
      "outputs": [],
      "source": [
        " import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8WI0DzdOIO3"
      },
      "outputs": [],
      "source": [
        "# Method 1: Using BitsAndBytes QAT (Recommended)\n",
        "def qat_with_bitsandbytes():\n",
        "    \"\"\"Quantization-Aware Training using BitsAndBytes\"\"\"\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        import bitsandbytes as bnb\n",
        "\n",
        "        # QAT Configuration\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "        model_name = \"distilgpt2\"\n",
        "\n",
        "        # Load model with quantization config\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Prepare for training\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model.enable_input_require_grads()\n",
        "\n",
        "        # Sample training data\n",
        "        texts = [\n",
        "            \"Quantization Aware Training helps reduce model size while maintaining performance.\",\n",
        "            \"Large Language Models can be efficiently quantized using various techniques.\",\n",
        "            \"QAT allows models to adapt to quantization during training process.\"\n",
        "        ]\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "        dataset = Dataset.from_dict({\"text\": texts})\n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./qat_model\",\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=3,\n",
        "            per_device_train_batch_size=1,\n",
        "            gradient_accumulation_steps=4,\n",
        "            warmup_steps=10,\n",
        "            logging_steps=1,\n",
        "            learning_rate=5e-5,\n",
        "            fp16=True,\n",
        "            save_strategy=\"no\",\n",
        "        )\n",
        "\n",
        "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        print(\"Starting QAT with BitsAndBytes...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Test the quantized model\n",
        "        test_input = \"Quantization makes models\"\n",
        "        inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=20, do_sample=True, temperature=0.7)\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            print(f\"Generated text: {generated_text}\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"BitsAndBytes not available. Install with: pip install bitsandbytes\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFik1EGJOQbP"
      },
      "outputs": [],
      "source": [
        "# Method 2: Custom QAT Implementation\n",
        "def custom_qat_llm():\n",
        "    \"\"\"Custom Quantization-Aware Training implementation\"\"\"\n",
        "\n",
        "    class QuantizedLinear(nn.Module):\n",
        "        def __init__(self, in_features, out_features, bias=True):\n",
        "            super().__init__()\n",
        "            self.in_features = in_features\n",
        "            self.out_features = out_features\n",
        "\n",
        "            # Learnable quantization parameters\n",
        "            self.weight_scale = nn.Parameter(torch.ones(1))\n",
        "            self.weight_zero_point = nn.Parameter(torch.zeros(1))\n",
        "            self.input_scale = nn.Parameter(torch.ones(1))\n",
        "            self.input_zero_point = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "            self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
        "            if bias:\n",
        "                self.bias = nn.Parameter(torch.randn(out_features))\n",
        "            else:\n",
        "                self.register_parameter('bias', None)\n",
        "\n",
        "        def quantize_tensor(self, tensor, scale, zero_point):\n",
        "            # Fake quantization during training\n",
        "            quantized = torch.round(tensor / scale + zero_point)\n",
        "            quantized = torch.clamp(quantized, -128, 127)  # int8 range\n",
        "            dequantized = (quantized - zero_point) * scale\n",
        "            return dequantized\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Quantize weights and inputs during forward pass\n",
        "            q_weight = self.quantize_tensor(self.weight, self.weight_scale, self.weight_zero_point)\n",
        "            q_input = self.quantize_tensor(x, self.input_scale, self.input_zero_point)\n",
        "            return nn.functional.linear(q_input, q_weight, self.bias)\n",
        "\n",
        "    def replace_linear_layers(model):\n",
        "        \"\"\"Replace Linear layers with Quantized versions\"\"\"\n",
        "        for name, module in model.named_children():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                # Replace with quantized version\n",
        "                quant_layer = QuantizedLinear(module.in_features, module.out_features,\n",
        "                                            module.bias is not None)\n",
        "                quant_layer.weight.data = module.weight.data.clone()\n",
        "                if module.bias is not None:\n",
        "                    quant_layer.bias.data = module.bias.data.clone()\n",
        "                setattr(model, name, quant_layer)\n",
        "            else:\n",
        "                replace_linear_layers(module)\n",
        "\n",
        "    model_name = \"distilgpt2\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Replace linear layers with quantized versions\n",
        "    print(\"Replacing linear layers with quantized versions...\")\n",
        "    replace_linear_layers(model)\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "    model.train()\n",
        "\n",
        "    training_texts = [\n",
        "        \"Quantization Aware Training on LLMs reduces model size significantly.\",\n",
        "        \"Custom QAT implementations allow fine control over quantization process.\",\n",
        "        \"Fake quantization during training helps models adapt to quantization noise.\"\n",
        "    ]\n",
        "\n",
        "    print(\"Starting custom QAT training...\")\n",
        "    for epoch in range(3):\n",
        "        total_loss = 0\n",
        "        for i, text in enumerate(training_texts):\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "            labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(training_texts)\n",
        "        print(f\"Epoch {epoch + 1} | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    test_input = \"Quantization helps\"\n",
        "    inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=15, do_sample=True, temperature=0.7)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Generated text: {generated_text}\")\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmQDhdlVOWY7"
      },
      "outputs": [],
      "source": [
        "# Method 3: Using PEFT with QLoRA (Most Practical)\n",
        "def qat_with_peft():\n",
        "    \"\"\"QAT using PEFT (Parameter Efficient Fine-Tuning) with QLoRA\"\"\"\n",
        "    try:\n",
        "        from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "        model_name = \"distilgpt2\"\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # LoRA configuration for QAT\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.1,\n",
        "            target_modules=[\"c_attn\", \"c_proj\"]  # DistilGPT2 specific\n",
        "        )\n",
        "\n",
        "        # Apply PEFT\n",
        "        model = get_peft_model(model, lora_config)\n",
        "        print(f\"Trainable parameters: {model.print_trainable_parameters()}\")\n",
        "\n",
        "        # Simple training\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "        model.train()\n",
        "\n",
        "        training_texts = [\n",
        "            \"PEFT with LoRA enables efficient quantization-aware training.\",\n",
        "            \"QLoRA combines quantization with low-rank adaptation effectively.\",\n",
        "            \"Parameter-efficient methods reduce memory usage during QAT.\"\n",
        "        ]\n",
        "\n",
        "        print(\"Starting PEFT QAT training...\")\n",
        "        for epoch in range(2):\n",
        "            total_loss = 0\n",
        "            for text in training_texts:\n",
        "                inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "                labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "                outputs = model(**inputs, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(training_texts)\n",
        "            print(f\"Epoch {epoch + 1} | Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Test\n",
        "        model.eval()\n",
        "        test_input = \"PEFT enables\"\n",
        "        inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=15, do_sample=True, temperature=0.7)\n",
        "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            print(f\"Generated text: {generated_text}\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"PEFT not available. Install with: pip install peft\")\n",
        "        return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfc7eVn4ti0g"
      },
      "outputs": [],
      "source": [
        "print(\"=== Quantization-Aware Training for LLMs ===\\n\")\n",
        "\n",
        "# Try Method 1: BitsAndBytes (most practical)\n",
        "print(\"Method 1: BitsAndBytes QAT\")\n",
        "model1, tokenizer1 = qat_with_bitsandbytes()\n",
        "\n",
        "if model1 is None:\n",
        "    print(\"\\nMethod 2: Custom QAT Implementation\")\n",
        "    model2, tokenizer2 = custom_qat_llm()\n",
        "\n",
        "    print(\"\\nMethod 3: PEFT with QLoRA\")\n",
        "    model3, tokenizer3 = qat_with_peft()\n",
        "\n",
        "print(\"\\nQAT implementation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sZMU3AkP1Un"
      },
      "source": [
        "What is True Quantization-Aware Training (QAT)?\n",
        "\n",
        "It simulates quantization during training using \"fake quantization\" modules (i.e., simulate low-precision behavior during forward pass but use full-precision weights for gradient updates).\n",
        "\n",
        "It trains both weights and quantizer parameters (like scale and zero-point).\n",
        "\n",
        "Mostly used in image models, but can be adapted for LLMs or Transformers with custom setup.\n",
        "\n",
        "Uses PyTorch's torch.ao.quantization or Brevitai / Intel Neural Compressor / NVIDIA toolkit.\n",
        "\n",
        "❌ Why it’s hard in LLMs?\n",
        "\n",
        "Hugging Face Transformers don’t directly support QAT yet for LLMs.\n",
        "\n",
        "No ready-made torch.ao.quantization.prepare_qat path for large nn.Module like GPT2, LLaMA, Falcon etc.\n",
        "\n",
        "But, for small models like DistilGPT2 or BERT, we can demo it.\n",
        "\n",
        "✅ Example: True QAT using PyTorch (Fake Quant + QConfig) on DistilGPT2\n",
        "\n",
        "This is a simplified setup using torch.ao.quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJsNxzBsQM_P"
      },
      "source": [
        "| QAT Method          | Models / Scale       | Key Benefit                                               |\n",
        "| ------------------- | -------------------- | --------------------------------------------------------- |\n",
        "| PyTorch QAT         | LLaMA‑3 8B           | Nearly PTQ-level accuracy with int4/int8 quantization     |\n",
        "| LLM‑QAT (Data‑Free) | LLaMA 7B / 13B / 30B | Better low-bit performance without requiring real data    |\n",
        "| EfficientQAT        | LLaMA‑2 up to 70B    | Full 2‑bit QAT on a single GPU with minimal accuracy loss |\n",
        "| DL‑QAT              | LLaMA / LLaMA‑2 (7B) | Efficient LoRA-style updates, stronger quality at 3-bit   |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1olH4Wv17y3w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH2HXhG7Qb2L"
      },
      "source": [
        "| Model Source / Project          | Provided Resources                                 |\n",
        "| ------------------------------- | -------------------------------------------------- |\n",
        "| **EfficientQAT** (OpenGVLab)    | Pre‑trained QAT LLaMA‑2 checkpoints (Hugging Face) |\n",
        "| **Gemma 3** (Google)            | QAT‑trained Gemma 3 variants publicly released     |\n",
        "| **LLM‑QAT** (Facebook Research) | Training code to produce your own QAT models       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzB95ZXXQ0Kb"
      },
      "source": [
        "google/gemma-3-1b-it-qat-q4_0-gguf\n",
        "\n",
        "google/gemma-3-4b-it-qat-q4_0-gguf\n",
        "\n",
        "google/gemma-3-12b-it-qat-q4_0-gguf\n",
        "\n",
        "google/gemma-3-27b-it-qat-q4_0-gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5waLQzfPPyZz"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "codP7nhlP9TX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.ao.quantization import (\n",
        "    QConfig,\n",
        "    fake_quantize_per_tensor_affine,\n",
        "    default_qat_qconfig,\n",
        "    prepare_qat,\n",
        "    convert,\n",
        ")\n",
        "\n",
        "# 1. Load a small model (DistilGPT2 for demo)\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# 2. Define a simple QConfig for QAT (you can tweak this)\n",
        "model.qconfig = default_qat_qconfig\n",
        "\n",
        "# 3. Prepare model for QAT (inserts fake quant nodes)\n",
        "model.train()\n",
        "qat_model = prepare_qat(model)\n",
        "\n",
        "# 4. Training loop (demo on toy data)\n",
        "text = \"Quantization aware training is powerful.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(qat_model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Starting QAT training...\")\n",
        "for epoch in range(3):\n",
        "    outputs = qat_model(**inputs, labels=inputs[\"input_ids\"])\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 5. Convert to real quantized model\n",
        "qat_model.eval()\n",
        "quantized_model = convert(qat_model)\n",
        "\n",
        "# 6. Test inference\n",
        "with torch.no_grad():\n",
        "    output = quantized_model.generate(**inputs, max_new_tokens=10)\n",
        "    print(\"\\nGenerated:\", tokenizer.decode(output[0]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}