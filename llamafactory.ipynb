{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBukQwDtiGPB",
    "outputId": "0e10f51c-31c5-4739-bb8e-204384050b94"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPBNia3W8A1P",
    "outputId": "3f403f46-ce46-4971-ee9e-7e89aae25f0d"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey1wi-IuiNXK",
    "outputId": "83c6e90b-89a1-48cf-f7fd-b03f96de47b7"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr0TVz8eifhW",
    "outputId": "a3f89ddd-e033-4cf6-b17c-f420b7828e49"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoCCNUPgimnk",
    "outputId": "3f3118a7-9ca8-4719-92b4-365849098638"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kg8J4red8S1j",
    "outputId": "bcd95d78-b82b-4d58-a22e-edbef968d886"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv1dXn6q8dzj"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes>=0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMaSCbvOQUFO",
    "outputId": "5b3efa76-7874-41e2-b08e-ee663da21301"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMwjgyWN8wiW",
    "outputId": "17e5b652-f8eb-4774-f974-f08d7529d412"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaeTAn1086Oi",
    "outputId": "4a8a543d-e3e5-4bb8-c375-2219ad238956"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GzTlLvieiNuv",
    "outputId": "9ebcdb5c-9e55-4ef0-8d2d-28825182a6b8"
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V5jvvkUYjoYJ",
    "outputId": "0c22d53a-9706-4847-da35-39aa713fd9a4"
   },
   "outputs": [],
   "source": [
    "!ls /content/LLaMA-Factory/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H709RivQnZUU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GRADIO_SHARE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75jh7-m1ig9u",
    "outputId": "80bfb7ea-64d1-4337-999b-ae48a8d33469"
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFoTL-suz1RT"
   },
   "outputs": [],
   "source": [
    "# !pip install accelerate transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKs2c58CmDVs",
    "outputId": "c8d4062d-bbe7-4e65-9030-489c0b6fb90c"
   },
   "outputs": [],
   "source": [
    "!python /content/LLaMA-Factory/src/webui.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz0Wt5K2paVL"
   },
   "outputs": [],
   "source": [
    "from llamafactory.webui.interface import create_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYThCYQNpcS5"
   },
   "outputs": [],
   "source": [
    "ui = create_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmMbSP8gpYpT"
   },
   "outputs": [],
   "source": [
    "ui.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqL6T1QdxEOr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Base model (same as you used in training)\n",
    "base_model = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "# Path to your trained LoRA adapter\n",
    "lora_path = \"/content/LLaMA-Factory/saves/Gemma-1.1-2B-Instruct/lora/train_2025-12-11-17-40-51\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4hC_QgdxP0T"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Apply LoRA on top of base model\n",
    "model = PeftModel.from_pretrained(model, lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0S9zXpTpkI4"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "\n",
    "\n",
    "# Enable evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# --- Inference prompt ---\n",
    "prompt = \"Can you tell what ls -l would display?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR40fbMfxwlX"
   },
   "source": [
    "##CLI Based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9ER9Xn0kAj-"
   },
   "outputs": [],
   "source": [
    "# !python -m llamafactory.cli train \\\n",
    "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
    "#   --template gemma \\\n",
    "#   --stage sft \\\n",
    "#   --finetuning_type lora \\\n",
    "#   --dataset yahma/alpaca-cleaned \\\n",
    "#   --output_dir output/my-gemma-qlora \\\n",
    "#   --cutoff_len 2048 \\\n",
    "#   --per_device_train_batch_size 1 \\\n",
    "#   --gradient_accumulation_steps 8 \\\n",
    "#   --num_train_epochs 1 \\\n",
    "#   --learning_rate 5e-5 \\\n",
    "#   --lora_rank 64 \\\n",
    "#   --lora_alpha 16 \\\n",
    "#   --lora_dropout 0.05 \\\n",
    "#   --quantization_bit 4 \\\n",
    "#   --fp16 True \\\n",
    "#   --gradient_checkpointing True \\\n",
    "#   --save_strategy epoch \\\n",
    "#   --save_steps 0 \\\n",
    "#   --save_total_limit 3 \\\n",
    "#   --logging_steps 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNQOeo0z887Z",
    "outputId": "ff5bf146-de4f-43ca-da41-a485075ef1f9"
   },
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSM0XV6Dvrnm",
    "outputId": "5a1e6bf2-5301-4e8f-d446-117c32f959ac"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6JDyHYpxswp",
    "outputId": "2437d5c0-b9c1-40b7-a7d6-3d4062277159"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYJ325HdH5Lx",
    "outputId": "fa450313-bf82-4f72-fe03-ca013fcc0484"
   },
   "outputs": [],
   "source": [
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66W9fQA0uIWM",
    "outputId": "e5c7ecc4-c231-4bfd-cd76-358d859cb773"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli train train_gemma_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OzqKWLcC7ol"
   },
   "outputs": [],
   "source": [
    "!ls output/my-gemma-qlora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OxnKVQhAM4V",
    "outputId": "fe1170fb-d992-4eeb-bd86-6f491687a2e0"
   },
   "outputs": [],
   "source": [
    "# !python -m llamafactory.cli train test.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KvNaOPnBbR2",
    "outputId": "adc6f80c-6fa4-4052-d3c5-9c52a05b96f0"
   },
   "outputs": [],
   "source": [
    "# !python -m llamafactory.cli train \\\n",
    "#   --model_name_or_path google/gemma-1.1-2b-it \\\n",
    "#   --template gemma \\\n",
    "#   --finetuning_type lora \\\n",
    "#   --dataset yahma/alpaca-cleaned \\\n",
    "#   --output_dir output/debug \\\n",
    "#   --per_device_train_batch_size 1 \\\n",
    "#   --gradient_accumulation_steps 8 \\\n",
    "#   --num_train_epochs 1 \\\n",
    "#   --lora_rank 32 --lora_alpha 16 --lora_dropout 0.05 \\\n",
    "#   --save_strategy epoch --save_total_limit 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cUT-URABqDK",
    "outputId": "1cf7704f-7f59-4423-dbb1-674f6fc8c61e"
   },
   "outputs": [],
   "source": [
    "# !df -h\n",
    "# !touch output/debug/testfile && ls output/debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUkmHvREASDZ"
   },
   "outputs": [],
   "source": [
    "!ls output/test-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RHYpdS2BESd",
    "outputId": "69dc4853-80b0-4dc3-be78-6d303687db70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"output/test-lora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475,
     "referenced_widgets": [
      "2a14756366f048c990c569f4e96794f3",
      "8f4ac9dde7344396b76036d964da31b9",
      "7386b31891024d6596ff5f6a32fa4cd0",
      "17bdcef1f0334a4681647fbbad3f3ef3",
      "e9f054691e9545e0ba627c7b33e33c98",
      "3fe013bf54824818b01676cb5842642e",
      "518d22c6903c42988786eefe77a8774a",
      "de57a09e83c34ab2baa9abfaeea6fe71",
      "296b464e196c4b2da2acd69501410f79",
      "4ec2cbe29f6343dd927c257b909524a6",
      "171e9377a40b4663a2f85c9215dd33a5"
     ]
    },
    "id": "H46HTzcvuVTQ",
    "outputId": "33bcc7e9-bf78-4208-ff37-032c88d9b1d5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base = \"google/gemma-1.1-2b-it\"\n",
    "adapter = \"/content/LLaMA-Factory/gemma_lora_sft_output\"   # your LoRA folder path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base)\n",
    "model = AutoModelForCausalLM.from_pretrained(base, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapter)\n",
    "\n",
    "prompt = \"Explain what is QLoRA\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGRj2u2a_Uxw"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIWGecfpuXWE"
   },
   "outputs": [],
   "source": [
    "!python -m llamafactory.cli export train_gemma_qlora.yaml\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
