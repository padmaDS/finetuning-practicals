{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-hBAvt-UiAn"
      },
      "outputs": [],
      "source": [
        "!pip -q install datasets accelerate transformers==4.56.2 trl==0.22.2 peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time, torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "t5cNI8RCXMvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "def prepare_dataset(n=200):\n",
        "    ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\").select(range(n))\n",
        "    def fmt(ex):\n",
        "        return {\"text\": alpaca_prompt.format(\n",
        "            instruction=ex[\"instruction\"],\n",
        "            input=ex[\"input\"]\n",
        "        ) + ex[\"output\"]}\n",
        "    return ds.map(fmt, remove_columns=ds.column_names)\n",
        "\n",
        "dataset = prepare_dataset()\n"
      ],
      "metadata": {
        "id": "-8WsPRxRXMx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ],
      "metadata": {
        "id": "eZwZJ-IvXM1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(\n",
        "    model,\n",
        "    LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\",\"v_proj\"],\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "KdnpwP7xXR4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.reset_peak_memory_stats()\n",
        "start = time.time()\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    args=SFTConfig(\n",
        "        max_steps=50,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        output_dir=\"hf_out\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "train_time = round(time.time() - start, 2)\n",
        "train_vram = round(torch.cuda.max_memory_reserved()/1024**3, 3)\n"
      ],
      "metadata": {
        "id": "Chm_UJD_XTKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Explain LoRA simply.\", return_tensors=\"pt\").to(\"cuda\")\n",
        "torch.cuda.synchronize()\n",
        "t0 = time.time()\n",
        "out = model.generate(**inputs, max_new_tokens=128)\n",
        "torch.cuda.synchronize()\n",
        "t1 = time.time()\n",
        "\n",
        "tokens_per_sec = round(out.shape[-1]/(t1 - t0), 2)\n",
        "\n",
        "print(\"HF RESULTS\")\n",
        "print(\"Train time (sec):\", train_time)\n",
        "print(\"Peak VRAM (GB):\", train_vram)\n",
        "print(\"Tokens/sec:\", tokens_per_sec)\n"
      ],
      "metadata": {
        "id": "0090G23CXUkt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}